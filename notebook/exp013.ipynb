{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback, log_summary\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# EXP_NAME = \"exp007\"\n",
    "EXP_NAME = \"debug\"\n",
    "\n",
    "class configs:\n",
    "    EXP_CATEGORY = \"baseline\"\n",
    "    EXP_NAME = EXP_NAME\n",
    "    OUTPUT_DIR = os.path.join(\"/workspace\", \"working\", EXP_NAME)\n",
    "    \n",
    "    INPUT_DIR = os.path.join(\"/workspace\", \"input\", \"atmaCup15_dataset\")\n",
    "    ORIG_TRAIN_CSV = os.path.join(INPUT_DIR, \"train.csv\")\n",
    "    ORIG_TEST_CSV = os.path.join(INPUT_DIR, \"test.csv\")\n",
    "    \n",
    "    TRAIN_CSV = \"/workspace/working/anime_svd/train_anime2vec.csv\"\n",
    "    ANIME_CSV = os.path.join(INPUT_DIR, \"anime.csv\")\n",
    "    TEST_CSV = \"/workspace/working/anime_svd/test_anime2vec.csv\"\n",
    "    SAMPLE_SUB_CSV = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n",
    "    target_colname = \"score\"\n",
    "    unused_cols = [\"fold\", \"score\", \"oof\", \"seen\"]\n",
    "    W2V_MODEL_PATH = \"/workspace/working/word2vec.gensim.model\"\n",
    "    \n",
    "    COMPETITION = \"atmaCup15\"\n",
    "    USER_NAME = \"taro\"\n",
    "    wandb_available = True\n",
    "    \n",
    "    # train\n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 200\n",
    "    verbose_eval = 500\n",
    "    FOLDS = [0, 1, 2, 3, 4]\n",
    "\n",
    "    TRAIN_BATCH_SIZE = 128\n",
    "    VALID_BATCH_SIZE = 128\n",
    "    EPOCHS = 30\n",
    "    # EPOCHS = 2\n",
    "    LEARNING_RATE = 1e-2\n",
    "\n",
    "\n",
    "if EXP_NAME is \"debug\":\n",
    "    configs.wandb_available = False\n",
    "    configs.num_boost_round = 10\n",
    "else:\n",
    "    os.makedirs(configs.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, logger=None, format_str=\"{:.3f}[s]\", prefix=None, suffix=None, sep=\" \"):\n",
    "\n",
    "        if prefix: format_str = str(prefix) + sep + format_str\n",
    "        if suffix: format_str = format_str + sep + str(suffix)\n",
    "        self.format_str = format_str\n",
    "        self.logger = logger\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end is None:\n",
    "            return 0\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time()\n",
    "        out_str = self.format_str.format(self.duration)\n",
    "        if self.logger:\n",
    "            self.logger.info(out_str)\n",
    "        else:\n",
    "            print(out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"mean_squared_error の root (0.5乗)\"\"\"\n",
    "    return mean_squared_error(y_true, y_pred) ** .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_by_anime_id(left_df, right_df):\n",
    "    return pd.merge(left_df[\"anime_id\"], right_df, on=\"anime_id\", how=\"left\").drop(columns=[\"anime_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_numeric_feature(input_df: pd.DataFrame):\n",
    "    \"\"\"input_dfは train or test.csv のデータが入ってくることを想定しています.\"\"\"\n",
    "    \n",
    "    use_columns = [\n",
    "        \"members\", \n",
    "    ]\n",
    "    \n",
    "    return merge_by_anime_id(input_df, anime_df)[use_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_genres_label_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"genres\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_le\": encoder.fit_transform(anime_df[target_col].fillna(\"nan\"))\n",
    "    })\n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_source_label_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"source\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_le\": encoder.fit_transform(anime_df[target_col].fillna(\"nan\"))\n",
    "    })\n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをカウントエンコーディング\n",
    "def create_anime_type_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"type\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_studios_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"studios\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_producers_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"producers\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_animeid_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"anime_id\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをone-hotエンコーディング\n",
    "def create_anime_type_one_hot_encoding(input_df: pd.DataFrame):\n",
    "    # 対象の列のユニーク集合を取る\n",
    "    target_colname = \"type\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    # ユニークな値ごとに列を作る\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        if value == \"Unknown\":\n",
    "            out_df[\"Unknown_type\"] = is_value.astype(int)\n",
    "        else:\n",
    "            out_df[value] = is_value.astype(int)\n",
    "    \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    return merge_by_anime_id(input_df, out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをone-hotエンコーディング\n",
    "def create_anime_rating_one_hot_encoding(input_df: pd.DataFrame):\n",
    "    # 対象の列のユニーク集合を取る\n",
    "    target_colname = \"rating\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    # ユニークな値ごとに列を作る\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        if value == \"Unknown\":\n",
    "            out_df[\"Unknown_rate\"] = is_value.astype(int)\n",
    "        else:\n",
    "            out_df[value] = is_value.astype(int)\n",
    "    \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    return merge_by_anime_id(input_df, out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上で定義した関数をまとめて実行\n",
    "def create_feature(input_df, config_):\n",
    "    # functions に特徴量作成関数を配列で定義しました.\n",
    "    # どの関数も同じ input / output のインターフェイスなので for で回せて嬉しいですね ;)\n",
    "    functions = [\n",
    "        create_anime_numeric_feature,\n",
    "        # label encoding\n",
    "        create_anime_genres_label_encoding, \n",
    "        create_anime_source_label_encoding, \n",
    "        # count encoding\n",
    "        # create_anime_type_count_encoding,\n",
    "        create_anime_studios_count_encoding,\n",
    "        create_anime_producers_count_encoding,\n",
    "        create_anime_animeid_count_encoding, \n",
    "        # one-hot encoding\n",
    "        create_anime_type_one_hot_encoding,\n",
    "        create_anime_rating_one_hot_encoding,\n",
    "    ]\n",
    "    \n",
    "    out_df = pd.DataFrame()\n",
    "    func_name_list = []\n",
    "    for func in functions:\n",
    "        func_name = str(func.__name__)\n",
    "        func_name_list.append(func_name)\n",
    "        with Timer(prefix=f\"create {func_name}\"):\n",
    "            _df = func(input_df)\n",
    "        out_df = pd.concat([out_df, _df], axis=1)\n",
    "    \n",
    "    config_.preprocess_funcs = func_name_list\n",
    "    return out_df, config_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, fold):\n",
    "    save_path = os.path.join(configs.OUTPUT_DIR, f\"model_fold{fold}.pkl\")\n",
    "    # pickle.dump(model, save_path)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"SAVED: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cls_to_reg(y_pred):\n",
    "#     # class予測を線形和で1-10に変換。\n",
    "#     return (y_pred * np.arange(1,11).reshape(1,10)).mean(axis=1)\n",
    "\n",
    "# def custom_loss(y_true, y_pred, alpha=0.5):\n",
    "#     \"\"\"\n",
    "#     y_true [batch]\n",
    "#     y_pred [batch, 10]\n",
    "#     \"\"\"\n",
    "#     y_true_onehot = np.eye(10)[y_true]\n",
    "#     ce_loss = (- y_true_onehot  * np.log(y_pred)).sum(axis=1).mean()\n",
    "#     # 回帰風ロスを追加で使ってもいい。\n",
    "#     y_pred_continuous = cls_to_reg(y_pred)\n",
    "#     mse_loss = ((y_true - y_pred_continuous )**2).mean()\n",
    "#     return ce_loss + alpha * mse_loss\n",
    "\n",
    "def cls_to_reg(y_pred):\n",
    "    # class予測を線形和で1-10に変換。\n",
    "    return (y_pred * torch.arange(1,11).view(1,10).to(device)).mean(axis=1)\n",
    "\n",
    "# def custom_loss(y_true, y_pred, alpha=0.5, smooth=1e-9):\n",
    "def custom_loss(y_pred, y_true, alpha=0.5, smooth=1e-9):\n",
    "    # y_true_cast = torch.tensor(y_true-1, dtype=torch.long)\n",
    "    # y_true_onehot = torch.eye(10).to(device)\n",
    "    # y_true_onehot = y_true_onehot[y_true_cast]\n",
    "    # y_true_cast = y_true_cast.to(device)\n",
    "    # ce_loss = (- y_true_onehot  * torch.log(y_pred + smooth)).sum(axis=1).mean()\n",
    "    CE_Loss = nn.CrossEntropyLoss()\n",
    "    ce_loss = CE_Loss(y_pred, torch.y_true)\n",
    "    y_pred_continuous = cls_to_reg(y_pred)\n",
    "    mse_loss = ((y_true - y_pred_continuous )**2).mean()\n",
    "    return ce_loss + alpha * mse_loss\n",
    "\n",
    "class CustomLoss():\n",
    "    def __init__(self, alpha=0.5, smooth=1e-9):\n",
    "        self.CE_LOSS = nn.CrossEntropyLoss()\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __call__(self, y_pred, y_true):\n",
    "        y_pred_continuous = cls_to_reg(y_pred)\n",
    "        ce_loss = self.CE_LOSS(y_pred, y_true).to(device)\n",
    "        mse_loss = ((y_true - y_pred_continuous )**2).mean()\n",
    "        return ce_loss + self.alpha * mse_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = self.X[idx] \n",
    "        labels = torch.tensor(self.y[idx] - 1, dtype=torch.long)\n",
    "        return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=32, output_size=10):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_nn_model(df):\n",
    "    models = []\n",
    "    evals_results_list = [] \n",
    "    n_records = len(df)\n",
    "    # training data の target と同じだけのゼロ配列を用意\n",
    "    oof_pred = np.zeros((n_records, ), dtype=np.float32)\n",
    "    target = []\n",
    "    for fold in configs.FOLDS: \n",
    "        # この部分が交差検証のところです。データセットを cv instance によって分割します\n",
    "        # training data を trian/valid に分割\n",
    "        train_df_ = df[df[\"fold\"] != fold].reset_index(drop=True)\n",
    "        valid_df_ = df[df[\"fold\"] == fold].reset_index(drop=True)\n",
    "        idx_valid = df[df[\"fold\"] == fold].index.values\n",
    "        \n",
    "        \n",
    "        feat_cols = [col for col in train_df_.columns if col not in configs.unused_cols]\n",
    "        print(\"feature nums = \", len(feat_cols))\n",
    "        \n",
    "        # dataset を作成\n",
    "        train_dataset = CustomDataset(train_df_[feat_cols], train_df_[configs.target_colname])         \n",
    "        valid_dataset = CustomDataset(valid_df_[feat_cols], valid_df_[configs.target_colname])         \n",
    "\n",
    "        # dataloader を作成\n",
    "        train_loader = DataLoader(train_dataset, batch_size=configs.TRAIN_BATCH_SIZE, shuffle=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=configs.VALID_BATCH_SIZE, shuffle=False)\n",
    "        \n",
    "        input_size = len(feat_cols)\n",
    "        model = SimpleNN(input_size)\n",
    "        model.to(device)\n",
    "        # criterion = nn.MSELoss()\n",
    "        # criterion = custom_loss\n",
    "        criterion = CustomLoss()\n",
    "        # criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=configs.LEARNING_RATE)\n",
    "        for epoch in range(configs.EPOCHS):\n",
    "            train_loss = 0.0\n",
    "            valid_loss = 0.0\n",
    "            model.train()\n",
    "            for i, (inputs, labels) in enumerate(train_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * inputs.size(0)\n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "\n",
    "            model.eval()\n",
    "            for i, (inputs, labels) in enumerate(valid_loader):\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item() * inputs.size(0)\n",
    "            valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "            if epoch % 10 == 0:            \n",
    "                print(f\"epoch {epoch} train_loss {train_loss:.4f} valid_loss {valid_loss:.4f}\")\n",
    "            # print(f\"epoch {epoch} train_loss {train_loss:.4f} valid_loss {valid_loss:.4f}\")\n",
    "\n",
    "\n",
    "        # この fold でのモデルを保存\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "        oof_preds, targets = [], []\n",
    "        # oof 予測\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                \n",
    "                outputs = torch.softmax(outputs, dim=1)\n",
    "                outputs = outputs.detach().cpu().numpy()\n",
    "                # 10class 分類の場合\n",
    "                outputs = np.argmax(outputs, axis=1)\n",
    "                oof_preds.extend(outputs.reshape(-1).tolist())\n",
    "                targets.extend(labels.detach().cpu().numpy().reshape(-1))\n",
    "        # 評価\n",
    "        rmse = root_mean_squared_error(targets, oof_preds)\n",
    "        print(f\"fold {fold}:  rmse {rmse}\")\n",
    "    \n",
    "        oof_pred[idx_valid] = oof_preds\n",
    "    # oof 予測値を保存\n",
    "    rmse = root_mean_squared_error(df[configs.target_colname], oof_pred)\n",
    "    print(f\"OVERALL: rmse {rmse}\")\n",
    "    return models, oof_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = pd.read_csv(configs.ANIME_CSV)\n",
    "\n",
    "train_df = pd.read_csv(configs.TRAIN_CSV)\n",
    "test_df = pd.read_csv(configs.TEST_CSV)\n",
    "\n",
    "original_train_df = pd.read_csv(configs.ORIG_TRAIN_CSV)\n",
    "original_test_df = pd.read_csv(configs.ORIG_TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_df, original_train_df, on=[\"user_id\", \"anime_id\"], how=\"left\")\n",
    "test_df = pd.merge(test_df, original_test_df, on=[\"user_id\", \"anime_id\"], how=\"left\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess\n",
    "- CountEncoding\n",
    "- OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create create_anime_numeric_feature 0.050[s]\n",
      "create create_anime_genres_label_encoding 0.015[s]\n",
      "create create_anime_source_label_encoding 0.013[s]\n",
      "create create_anime_studios_count_encoding 0.013[s]\n",
      "create create_anime_producers_count_encoding 0.013[s]\n",
      "create create_anime_animeid_count_encoding 0.011[s]\n",
      "create create_anime_type_one_hot_encoding 0.015[s]\n",
      "create create_anime_rating_one_hot_encoding 0.018[s]\n",
      "train... 0.158[s]\n",
      "create create_anime_numeric_feature 0.040[s]\n",
      "create create_anime_genres_label_encoding 0.014[s]\n",
      "create create_anime_source_label_encoding 0.011[s]\n",
      "create create_anime_studios_count_encoding 0.011[s]\n",
      "create create_anime_producers_count_encoding 0.011[s]\n",
      "create create_anime_animeid_count_encoding 0.015[s]\n",
      "create create_anime_type_one_hot_encoding 0.017[s]\n",
      "create create_anime_rating_one_hot_encoding 0.016[s]\n",
      "test... 0.142[s]\n"
     ]
    }
   ],
   "source": [
    "# 実行して train / test 用の特徴量を作ります.\n",
    "\n",
    "with Timer(prefix=\"train...\"):\n",
    "    train_feat_df, configs = create_feature(train_df, configs)\n",
    "\n",
    "with Timer(prefix=\"test...\"):\n",
    "    test_feat_df, configs = create_feature(test_df, configs)\n",
    "\n",
    "# X = train_feat_df.values\n",
    "# print(train_feat_df.columns)\n",
    "# y = train_df[\"score\"].values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_by_anime_id(left_df, right_df):\n",
    "    return pd.merge(left_df[\"anime_id\"], right_df, on=\"anime_id\", how=\"left\").drop(columns=[\"anime_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_numeric_feature(input_df: pd.DataFrame):\n",
    "    \"\"\"input_dfは train or test.csv のデータが入ってくることを想定しています.\"\"\"\n",
    "    \n",
    "    use_columns = [\n",
    "        \"members\", \n",
    "    ]\n",
    "    \n",
    "    return merge_by_anime_id(input_df, anime_df)[use_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_genres_label_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"genres\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_le\": encoder.fit_transform(anime_df[target_col].fillna(\"nan\"))\n",
    "    })\n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_source_label_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"source\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_le\": encoder.fit_transform(anime_df[target_col].fillna(\"nan\"))\n",
    "    })\n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをカウントエンコーディング\n",
    "def create_anime_type_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"type\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_studios_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"studios\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_producers_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"producers\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_animeid_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"anime_id\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをone-hotエンコーディング\n",
    "def create_anime_type_one_hot_encoding(input_df: pd.DataFrame):\n",
    "    # 対象の列のユニーク集合を取る\n",
    "    target_colname = \"type\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    # ユニークな値ごとに列を作る\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        if value == \"Unknown\":\n",
    "            out_df[\"Unknown_type\"] = is_value.astype(int)\n",
    "        else:\n",
    "            out_df[value] = is_value.astype(int)\n",
    "    \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    return merge_by_anime_id(input_df, out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをone-hotエンコーディング\n",
    "def create_anime_rating_one_hot_encoding(input_df: pd.DataFrame):\n",
    "    # 対象の列のユニーク集合を取る\n",
    "    target_colname = \"rating\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    # ユニークな値ごとに列を作る\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        if value == \"Unknown\":\n",
    "            out_df[\"Unknown_rate\"] = is_value.astype(int)\n",
    "        else:\n",
    "            out_df[value] = is_value.astype(int)\n",
    "    \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    return merge_by_anime_id(input_df, out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上で定義した関数をまとめて実行\n",
    "def create_feature(input_df, config_):\n",
    "    # functions に特徴量作成関数を配列で定義しました.\n",
    "    # どの関数も同じ input / output のインターフェイスなので for で回せて嬉しいですね ;)\n",
    "    functions = [\n",
    "        create_anime_numeric_feature,\n",
    "        # label encoding\n",
    "        create_anime_genres_label_encoding, \n",
    "        create_anime_source_label_encoding, \n",
    "        # count encoding\n",
    "        # create_anime_type_count_encoding,\n",
    "        create_anime_studios_count_encoding,\n",
    "        create_anime_producers_count_encoding,\n",
    "        create_anime_animeid_count_encoding, \n",
    "        # one-hot encoding\n",
    "        create_anime_type_one_hot_encoding,\n",
    "        create_anime_rating_one_hot_encoding,\n",
    "    ]\n",
    "    \n",
    "    out_df = pd.DataFrame()\n",
    "    func_name_list = []\n",
    "    for func in functions:\n",
    "        func_name = str(func.__name__)\n",
    "        func_name_list.append(func_name)\n",
    "        with Timer(prefix=f\"create {func_name}\"):\n",
    "            _df = func(input_df)\n",
    "        out_df = pd.concat([out_df, _df], axis=1)\n",
    "    \n",
    "    config_.preprocess_funcs = func_name_list\n",
    "    return out_df, config_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.concat([train_df, train_feat_df], axis=1)\n",
    "input_df = input_df.drop([\"user_id\", \"anime_id\"], axis=1)\n",
    "input_df = input_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['wo_score_user_factor_0', 'wo_score_user_factor_1',\n",
       "       'wo_score_user_factor_2', 'wo_score_user_factor_3',\n",
       "       'wo_score_user_factor_4', 'wo_score_user_factor_5',\n",
       "       'wo_score_user_factor_6', 'wo_score_user_factor_7',\n",
       "       'wo_score_user_factor_8', 'wo_score_user_factor_9',\n",
       "       'wo_score_user_factor_10', 'wo_score_user_factor_11',\n",
       "       'wo_score_user_factor_12', 'wo_score_user_factor_13',\n",
       "       'wo_score_user_factor_14', 'wo_score_user_factor_15',\n",
       "       'wo_score_user_factor_16', 'wo_score_user_factor_17',\n",
       "       'wo_score_user_factor_18', 'wo_score_user_factor_19',\n",
       "       'wo_score_user_factor_20', 'wo_score_user_factor_21',\n",
       "       'wo_score_user_factor_22', 'wo_score_user_factor_23',\n",
       "       'wo_score_user_factor_24', 'wo_score_user_factor_25',\n",
       "       'wo_score_user_factor_26', 'wo_score_user_factor_27',\n",
       "       'wo_score_user_factor_28', 'wo_score_user_factor_29',\n",
       "       'wo_score_user_factor_30', 'wo_score_user_factor_31',\n",
       "       'wo_score_user_factor_32', 'wo_score_user_factor_33',\n",
       "       'wo_score_user_factor_34', 'wo_score_user_factor_35',\n",
       "       'wo_score_user_factor_36', 'wo_score_user_factor_37',\n",
       "       'wo_score_user_factor_38', 'wo_score_user_factor_39',\n",
       "       'wo_score_user_factor_40', 'wo_score_user_factor_41',\n",
       "       'wo_score_user_factor_42', 'wo_score_user_factor_43',\n",
       "       'wo_score_user_factor_44', 'wo_score_user_factor_45',\n",
       "       'wo_score_user_factor_46', 'wo_score_user_factor_47',\n",
       "       'wo_score_user_factor_48', 'wo_score_user_factor_49',\n",
       "       'wo_score_user_factor_50', 'wo_score_user_factor_51',\n",
       "       'wo_score_user_factor_52', 'wo_score_user_factor_53',\n",
       "       'wo_score_user_factor_54', 'wo_score_user_factor_55',\n",
       "       'wo_score_user_factor_56', 'wo_score_user_factor_57',\n",
       "       'wo_score_user_factor_58', 'wo_score_user_factor_59',\n",
       "       'wo_score_user_factor_60', 'wo_score_user_factor_61',\n",
       "       'wo_score_user_factor_62', 'wo_score_user_factor_63', 'oof', 'seen',\n",
       "       'fold', 'svd', 'score', 'members', 'genres_le', 'source_le',\n",
       "       'studios_count', 'producers_count', 'anime_id_count', 'TV', 'Special',\n",
       "       'Movie', 'Unknown_type', 'ONA', 'OVA', 'Music',\n",
       "       'PG-13 - Teens 13 or older', 'R+ - Mild Nudity',\n",
       "       'R - 17+ (violence & profanity)', 'G - All Ages', 'PG - Children',\n",
       "       'Rx - Hentai', 'Unknown_rate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature nums =  85\n",
      "epoch 0 train_loss 5065508.8644 valid_loss 10532.0888\n",
      "epoch 10 train_loss 3.8804 valid_loss 3.4119\n",
      "epoch 20 train_loss 3.0082 valid_loss 3.0079\n",
      "fold 0:  rmse 1.5794281581309637\n",
      "feature nums =  85\n",
      "epoch 0 train_loss 5899303.8610 valid_loss 4093.3482\n",
      "epoch 10 train_loss 3.4508 valid_loss 3.3833\n",
      "epoch 20 train_loss 3.0079 valid_loss 3.0075\n",
      "fold 1:  rmse 1.5793642694455174\n",
      "feature nums =  85\n",
      "epoch 0 train_loss 1907817.3848 valid_loss 3923.4999\n",
      "epoch 10 train_loss 3.3549 valid_loss 3.3127\n",
      "epoch 20 train_loss 3.0077 valid_loss 3.0065\n",
      "fold 2:  rmse 1.5793526644513813\n",
      "feature nums =  85\n",
      "epoch 0 train_loss 7334626.8646 valid_loss 2815.1057\n",
      "epoch 10 train_loss 108.4727 valid_loss 114.6432\n",
      "epoch 20 train_loss 3.0100 valid_loss 3.0093\n",
      "fold 3:  rmse 1.5795963514237064\n",
      "feature nums =  85\n",
      "epoch 0 train_loss 5564094.3717 valid_loss 18567.3366\n",
      "epoch 10 train_loss 4.2503 valid_loss 3.8570\n",
      "epoch 20 train_loss 3.0095 valid_loss 3.0094\n",
      "fold 4:  rmse 1.5795963514237064\n",
      "OVERALL: rmse {rmse}\n"
     ]
    }
   ],
   "source": [
    "models, oof_pred = fit_nn_model(input_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['svd', 'members', 'tyoe_count', 'TV', 'Special', 'Movie', 'Unknown','ONA', 'OVA', 'Music']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feat_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([test_df, test_feat_df], axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m test_df \u001b[39m=\u001b[39m test_df[feat_cols]\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(test_df\u001b[39m.\u001b[39mcolumns)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(test_df\u001b[39m.\u001b[39mcolumns))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feat_cols' is not defined"
     ]
    }
   ],
   "source": [
    "test_df = pd.concat([test_df, test_feat_df], axis=1)\n",
    "test_df = test_df[feat_cols]\n",
    "print(test_df.columns)\n",
    "print(len(test_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k 個のモデルの予測を作成. shape = (5, N_test,).\n",
    "pred = np.array([model.predict(test_df.values) for model in models])\n",
    "\n",
    "\n",
    "# k 個のモデルの予測値の平均 shape = (N_test,).\n",
    "pred = np.mean(pred, axis=0) # axis=0 なので shape の `k` が潰れる "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "vmax = .02\n",
    "bins = np.linspace(0, 10, 100)\n",
    "ax.hist(pred, bins=bins, density=True, alpha=.5, label=\"Test\")\n",
    "ax.hist(oof, bins=bins, density=True, alpha=.5, label=\"OutOfFold\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "# ax.set_title(\"テストと学習時の予測傾向差分\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save infer csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"score\": pred\n",
    "}).to_csv(os.path.join(configs.OUTPUT_DIR, \"submission.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
