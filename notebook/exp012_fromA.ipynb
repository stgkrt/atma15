{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from surprise import Dataset, Reader, SVDpp\n",
    "\n",
    "\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_w2v_features_with_score(train_df, val_df, test_df=None):\n",
    "    anime_ids = train_df['anime_id'].unique().tolist()\n",
    "    user_anime_list_dict = {user_id: anime_ids.tolist() for user_id, anime_ids in train_df.groupby('user_id')['anime_id']}\n",
    "\n",
    "    # スコアを考慮する場合\n",
    "    # 今回は1～10のレーティングなので、スコアが5のアニメは5回、スコアが10のアニメは10回、タイトルをリストに追加する\n",
    "    title_sentence_list = []\n",
    "    for user_id, user_df in train_df.groupby('user_id'):\n",
    "        user_title_sentence_list = []\n",
    "        for anime_id, anime_score in user_df[['anime_id', 'score']].values:\n",
    "            for i in range(anime_score):\n",
    "                user_title_sentence_list.append(anime_id)\n",
    "        title_sentence_list.append(user_title_sentence_list)\n",
    "\n",
    "    # ユーザごとにshuffleしたリストを作成\n",
    "    shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in title_sentence_list]  ## <= 変更点\n",
    "\n",
    "    # 元のリストとshuffleしたリストを合わせる\n",
    "    train_sentence_list = title_sentence_list + shuffled_sentence_list\n",
    "\n",
    "    # word2vecのパラメータ\n",
    "    vector_size = 64\n",
    "    w2v_params = {\n",
    "        \"vector_size\": vector_size,  ## <= 変更点\n",
    "        \"seed\": SEED,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 1\n",
    "    }\n",
    "\n",
    "    # word2vecのモデル学習\n",
    "    model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n",
    "\n",
    "    # ユーザーごとの特徴ベクトルと対応するユーザーID\n",
    "    user_factors = {user_id: np.mean([model.wv[anime_id] for anime_id in user_anime_list], axis=0) for user_id, user_anime_list in user_anime_list_dict.items()}\n",
    "\n",
    "    # アイテムごとの特徴ベクトルと対応するアイテムID\n",
    "    item_factors = {aid: model.wv[aid] for aid in anime_ids}\n",
    "\n",
    "    # データフレームを作成\n",
    "    user_factors_df = pd.DataFrame(user_factors).T.reset_index().rename(columns={\"index\": \"user_id\"})\n",
    "    item_factors_df = pd.DataFrame(item_factors).T.reset_index().rename(columns={\"index\": \"anime_id\"})\n",
    "\n",
    "    # データフレームのカラム名をリネーム\n",
    "    user_factors_df.columns = [\"user_id\"] + [f\"user_factor_{i}\" for i in range(vector_size)]\n",
    "    item_factors_df.columns = [\"anime_id\"] + [f\"item_factor_{i}\" for i in range(vector_size)]\n",
    "\n",
    "    train_df = train_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "    train_df = train_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n",
    "\n",
    "    val_df = val_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "    val_df = val_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n",
    "\n",
    "    if test_df is not None:\n",
    "        test_df = test_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "        test_df = test_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_w2v_features_without_score(train_test_df):\n",
    "    \n",
    "    anime_ids = train_test_df['anime_id'].unique().tolist()\n",
    "    user_anime_list_dict = {user_id: anime_ids.tolist() for user_id, anime_ids in train_test_df.groupby('user_id')['anime_id']}\n",
    "\n",
    "    title_sentence_list = train_test_df.groupby('user_id')['anime_id'].apply(list).tolist()\n",
    "\n",
    "    # ユーザごとにshuffleしたリストを作成\n",
    "    shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in title_sentence_list]  ## <= 変更点\n",
    "\n",
    "    # 元のリストとshuffleしたリストを合わせる\n",
    "    train_sentence_list = title_sentence_list + shuffled_sentence_list\n",
    "\n",
    "    # word2vecのパラメータ\n",
    "    vector_size = 64\n",
    "    w2v_params = {\n",
    "        \"vector_size\": vector_size,  ## <= 変更点\n",
    "        \"seed\": SEED,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 1\n",
    "    }\n",
    "\n",
    "    # word2vecのモデル学習\n",
    "    model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n",
    "\n",
    "    # ユーザーごとの特徴ベクトルと対応するユーザーID\n",
    "    user_factors = {user_id: np.mean([model.wv[anime_id] for anime_id in user_anime_list], axis=0) for user_id, user_anime_list in user_anime_list_dict.items()}\n",
    "\n",
    "    # アイテムごとの特徴ベクトルと対応するアイテムID\n",
    "    item_factors = {aid: model.wv[aid] for aid in anime_ids}\n",
    "\n",
    "    # データフレームを作成\n",
    "    user_factors_df = pd.DataFrame(user_factors).T.reset_index().rename(columns={\"index\": \"user_id\"})\n",
    "    item_factors_df = pd.DataFrame(item_factors).T.reset_index().rename(columns={\"index\": \"anime_id\"})\n",
    "\n",
    "    # データフレームのカラム名をリネーム\n",
    "    user_factors_df.columns = [\"user_id\"] + [f\"wo_score_user_factor_{i}\" for i in range(vector_size)]\n",
    "    item_factors_df.columns = [\"anime_id\"] + [f\"wo_score_item_factor_{i}\" for i in range(vector_size)]\n",
    "\n",
    "    train_test_df = train_test_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "\n",
    "    return train_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_by_anime_id(left_df, right_df):\n",
    "    return pd.merge(left_df[\"anime_id\"], right_df, on=\"anime_id\", how=\"left\").drop(columns=[\"anime_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_numeric_feature(input_df: pd.DataFrame):\n",
    "    \"\"\"input_dfは train or test.csv のデータが入ってくることを想定しています.\"\"\"\n",
    "    \n",
    "    use_columns = [\n",
    "        \"members\", \n",
    "    ]\n",
    "    \n",
    "    return merge_by_anime_id(input_df, anime_df)[use_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_genres_label_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"genres\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_le\": encoder.fit_transform(anime_df[target_col].fillna(\"nan\"))\n",
    "    })\n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_source_label_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"source\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_le\": encoder.fit_transform(anime_df[target_col].fillna(\"nan\"))\n",
    "    })\n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをカウントエンコーディング\n",
    "def create_anime_type_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"type\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_studios_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"studios\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_producers_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"producers\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_animeid_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"anime_id\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをone-hotエンコーディング\n",
    "def create_anime_type_one_hot_encoding(input_df: pd.DataFrame):\n",
    "    # 対象の列のユニーク集合を取る\n",
    "    target_colname = \"type\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    # ユニークな値ごとに列を作る\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        if value == \"Unknown\":\n",
    "            out_df[\"Unknown_type\"] = is_value.astype(int)\n",
    "        else:\n",
    "            out_df[value] = is_value.astype(int)\n",
    "    \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    return merge_by_anime_id(input_df, out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをone-hotエンコーディング\n",
    "def create_anime_rating_one_hot_encoding(input_df: pd.DataFrame):\n",
    "    # 対象の列のユニーク集合を取る\n",
    "    target_colname = \"rating\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    # ユニークな値ごとに列を作る\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        if value == \"Unknown\":\n",
    "            out_df[\"Unknown_rate\"] = is_value.astype(int)\n",
    "        else:\n",
    "            out_df[value] = is_value.astype(int)\n",
    "    \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    return merge_by_anime_id(input_df, out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上で定義した関数をまとめて実行\n",
    "def create_feature(input_df):\n",
    "    # functions に特徴量作成関数を配列で定義しました.\n",
    "    # どの関数も同じ input / output のインターフェイスなので for で回せて嬉しいですね ;)\n",
    "    functions = [\n",
    "        create_anime_numeric_feature,\n",
    "        # label encoding\n",
    "        create_anime_genres_label_encoding, \n",
    "        create_anime_source_label_encoding, \n",
    "        # count encoding\n",
    "        create_anime_type_count_encoding,\n",
    "        create_anime_studios_count_encoding,\n",
    "        create_anime_producers_count_encoding,\n",
    "        create_anime_animeid_count_encoding, \n",
    "        # one-hot encoding\n",
    "        create_anime_type_one_hot_encoding,\n",
    "        create_anime_rating_one_hot_encoding,\n",
    "    ]\n",
    "    \n",
    "    out_df = pd.DataFrame()\n",
    "    func_name_list = []\n",
    "    for func in functions:\n",
    "        func_name = str(func.__name__)\n",
    "        func_name_list.append(func_name)\n",
    "        with timer(f\"create {func_name}\"):\n",
    "            _df = func(input_df)\n",
    "        out_df = pd.concat([out_df, _df], axis=1)\n",
    "    \n",
    "    return out_df "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習に便利な関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_df = pd.read_csv('/workspace/input/atmaCup15_dataset/train.csv')\n",
    "    test_df = pd.read_csv('/workspace/input/atmaCup15_dataset/test.csv')\n",
    "    test_df['score'] = 0 # dummy\n",
    "\n",
    "    # Initialize submission file\n",
    "    submission_df = pd.read_csv('/workspace/input/atmaCup15_dataset/sample_submission.csv')\n",
    "    submission_df['score'] = 0\n",
    "    return train_df, test_df, submission_df\n",
    "\n",
    "def stratified_and_group_kfold_split(train_df):\n",
    "    # https://www.guruguru.science/competitions/21/discussions/45ffc8a1-e37c-4b95-aac4-c4e338aa6a9b/\n",
    "    \n",
    "    # 20%のユーザを抽出\n",
    "    n_user = train_df[\"user_id\"].nunique()\n",
    "    unseen_users = random.sample(sorted(train_df[\"user_id\"].unique()), k=n_user // 5)\n",
    "    train_df[\"unseen_user\"] = train_df[\"user_id\"].isin(unseen_users)\n",
    "    unseen_df = train_df[train_df[\"unseen_user\"]].reset_index(drop=True)\n",
    "    train_df = train_df[~train_df[\"unseen_user\"]].reset_index(drop=True)\n",
    "\n",
    "    # train_dfの80%をStratifiedKFoldで分割\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    for fold_id, (_, valid_idx) in enumerate(skf.split(train_df, train_df[\"user_id\"])):\n",
    "        train_df.loc[valid_idx, \"fold\"] = fold_id\n",
    "\n",
    "    # 20%をGroupKFoldで分割\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    unseen_df[\"fold\"] = -1\n",
    "    for fold_id, (_, valid_idx) in enumerate(gkf.split(unseen_df, unseen_df[\"user_id\"], unseen_df[\"user_id\"])):\n",
    "        unseen_df.loc[valid_idx, \"fold\"] = fold_id\n",
    "\n",
    "    # concat\n",
    "    train_df = pd.concat([train_df, unseen_df], axis=0).reset_index(drop=True)\n",
    "    train_df.drop(columns=[\"unseen_user\"], inplace=True)\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def train(train_df, original_test_df, submission_df):\n",
    "    train_df['oof'] = 0\n",
    "    train_df['seen'] = False\n",
    "\n",
    "    for fold in range(5):\n",
    "        # Prepare the train and validation data\n",
    "        trn_df = train_df[train_df['fold'] != fold].copy()\n",
    "        val_df = train_df[train_df['fold'] == fold].copy()\n",
    "\n",
    "        trn_df, val_df, test_df = add_w2v_features_with_score(trn_df, val_df, original_test_df.copy())\n",
    "        \n",
    "        # Define the features and the target\n",
    "        unused_cols = ['user_id', 'anime_id', 'score', 'fold', 'oof', 'seen']\n",
    "        feature_cols = [col for col in trn_df.columns if col not in unused_cols]\n",
    "        target_col = 'score'\n",
    "\n",
    "        # Prepare the LightGBM datasets\n",
    "        lgb_train = lgb.Dataset(trn_df[feature_cols], trn_df[target_col])\n",
    "        lgb_val = lgb.Dataset(val_df[feature_cols], val_df[target_col])\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': 0.1,\n",
    "            # 'reg_lambda': 1.0\n",
    "        }\n",
    "\n",
    "        # Train the model\n",
    "        callbacks = [\n",
    "            lgb.early_stopping(stopping_rounds=200),\n",
    "            lgb.log_evaluation(period=500)\n",
    "        ]\n",
    "        model_lgb = lgb.train(params,\n",
    "                              lgb_train,\n",
    "                              valid_sets=[lgb_train, lgb_val],\n",
    "                              callbacks=callbacks,\n",
    "                              num_boost_round=10000)\n",
    "        # Predict\n",
    "        trn_df['preds'] = model_lgb.predict(trn_df[feature_cols], num_iteration=model_lgb.best_iteration)\n",
    "        val_df['preds'] = model_lgb.predict(val_df[feature_cols], num_iteration=model_lgb.best_iteration)\n",
    "        test_preds = model_lgb.predict(test_df[feature_cols], num_iteration=model_lgb.best_iteration)\n",
    "\n",
    "        train_users = trn_df['user_id'].unique()\n",
    "        is_seen = val_df['user_id'].isin(train_users)\n",
    "        seen_val = val_df[is_seen]\n",
    "        unseen_val = val_df[~is_seen]\n",
    "\n",
    "        # Evaluate the model\n",
    "        train_score = np.sqrt(mean_squared_error(trn_df['score'], trn_df['preds']))\n",
    "        seen_val_score = np.sqrt(mean_squared_error(seen_val['score'], seen_val['preds']))\n",
    "        unseen_val_score = np.sqrt(mean_squared_error(unseen_val['score'], unseen_val['preds']))\n",
    "        print(f'fold{fold} train RMSE: {train_score:.3f}, seen val RMSE: {seen_val_score:.3f}, unseen val RMSE: {unseen_val_score:.3f}')\n",
    "        \n",
    "        submission_df['score'] += test_preds / 5\n",
    "\n",
    "        train_df.loc[train_df['fold'] == fold, 'oof'] = val_df['preds'].values\n",
    "        train_df.loc[train_df['fold'] == fold, 'seen'] = is_seen.values\n",
    "\n",
    "    total_score = np.sqrt(mean_squared_error(train_df['score'], train_df['oof']))\n",
    "    seen_score = np.sqrt(mean_squared_error(train_df[train_df['seen']]['score'], train_df[train_df['seen']]['oof']))\n",
    "    unseen_score = np.sqrt(mean_squared_error(train_df[~train_df['seen']]['score'], train_df[~train_df['seen']]['oof']))\n",
    "    print(f\"Total RMSE: {total_score} | Seen RMSE: {seen_score} | Unseen RMSE: {unseen_score}\")\n",
    "\n",
    "    # train_df.to_csv(os.path.join(\"workspace\", \"working\", \"anime2vec\",'train_anime2vec.csv'), index=False)\n",
    "    # submission_df.to_csv(os.path.join(\"workspace\", \"working\", \"anime2vec\",'submission.csv'), index=False)o\n",
    "    return train_df, test_df, submission_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load the data] done in 0 s\n",
      "[Stratified & Group split] done in 0 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[add_w2v_features_without_score] done in 2 s\n",
      "[create create_anime_numeric_feature] done in 0 s\n",
      "[create create_anime_genres_label_encoding] done in 0 s\n",
      "[create create_anime_source_label_encoding] done in 0 s\n",
      "[create create_anime_type_count_encoding] done in 0 s\n",
      "[create create_anime_studios_count_encoding] done in 0 s\n",
      "[create create_anime_producers_count_encoding] done in 0 s\n",
      "[create create_anime_animeid_count_encoding] done in 0 s\n",
      "[create create_anime_type_one_hot_encoding] done in 0 s\n",
      "[create create_anime_rating_one_hot_encoding] done in 0 s\n",
      "[train...] done in 0 s\n",
      "[create create_anime_numeric_feature] done in 0 s\n",
      "[create create_anime_genres_label_encoding] done in 0 s\n",
      "[create create_anime_source_label_encoding] done in 0 s\n",
      "[create create_anime_type_count_encoding] done in 0 s\n",
      "[create create_anime_studios_count_encoding] done in 0 s\n",
      "[create create_anime_producers_count_encoding] done in 0 s\n",
      "[create create_anime_animeid_count_encoding] done in 0 s\n",
      "[create create_anime_type_one_hot_encoding] done in 0 s\n",
      "[create create_anime_rating_one_hot_encoding] done in 0 s\n",
      "[test...] done in 0 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Load the data\"):\n",
    "    train_df, test_df, submission_df = load_data()\n",
    "    anime_df = pd.read_csv(\"/workspace/input/atmaCup15_dataset/anime.csv\")\n",
    "with timer(\"Stratified & Group split\"):\n",
    "    train_df = stratified_and_group_kfold_split(train_df)\n",
    "\n",
    "with timer(\"add_w2v_features_without_score\"):\n",
    "    # testの視聴情報も活用するため、trainとtestを結合して先に特徴量を作成\n",
    "    train_test_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "    train_test_df = add_w2v_features_without_score(train_test_df)\n",
    "    train_df = train_test_df[train_test_df['score'] != 0].copy().reset_index(drop=True)\n",
    "    test_df = train_test_df[train_test_df['score'] == 0].copy().reset_index(drop=True)\n",
    "\n",
    "with timer(\"train...\"):\n",
    "    train_feat_df = create_feature(train_df)\n",
    "    train_df = pd.concat([train_df, train_feat_df], axis=1)\n",
    "\n",
    "with timer(\"test...\"):\n",
    "    test_feat_df = create_feature(test_df)\n",
    "    test_df = pd.concat([test_df, test_feat_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "fold 1\n",
      "fold 2\n",
      "fold 3\n",
      "fold 4\n",
      "54561\n",
      "81841\n",
      "109121\n",
      "136401\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = os.path.join(\"/workspace\", \"input\", \"atmaCup15_dataset\")\n",
    "class configs:\n",
    "    INPUT_DIR = INPUT_DIR\n",
    "    NPUT_DIR = os.path.join(\"/workspace\", \"input\", \"atmaCup15_dataset\")\n",
    "    TRAIN_CSV = os.path.join(INPUT_DIR, \"train_stratifiedgroupkfold.csv\")\n",
    "    ANIME_CSV = os.path.join(INPUT_DIR, \"anime.csv\")\n",
    "    TEST_CSV = os.path.join(INPUT_DIR, \"test.csv\")\n",
    "    SAMPLE_SUB_CSV = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n",
    "    SEED = 42\n",
    "\n",
    "# Load the dataset\n",
    "train_df = pd.read_csv(os.path.join(configs.INPUT_DIR, \"train_stratifiedkfold.csv\"))\n",
    "reader = Reader(rating_scale=(1, 10))\n",
    "\n",
    "models = []\n",
    "for fold in sorted(train_df[\"fold\"].unique()):\n",
    "    print(\"fold\", fold)\n",
    "    train_df_ = train_df[train_df[\"fold\"] != fold].reset_index(drop=True)\n",
    "    train_data = Dataset.load_from_df(train_df_[['user_id', 'anime_id', 'score']], reader)\n",
    "    model = SVDpp()\n",
    "    model.fit(train_data.build_full_trainset())\n",
    "    models.append(model)\n",
    "\n",
    "    \n",
    "oof_df = pd.DataFrame()\n",
    "for fold, model in enumerate(models):\n",
    "    test_df_ = train_df[train_df[\"fold\"] == fold].reset_index(drop=True)\n",
    "    test_data = Dataset.load_from_df(test_df_[['user_id', 'anime_id', 'score']], reader)\n",
    "    oof_pred = model.test(test_data.build_full_trainset().build_testset())\n",
    "    pred = [pred.est for pred in oof_pred]\n",
    "    # predictions.extend(oof_pred)\n",
    "    if len(oof_df)==0:\n",
    "        test_df_[\"svd\"] = pred\n",
    "        oof_df = test_df_\n",
    "    else:\n",
    "        test_df_[\"svd\"] = pred\n",
    "        oof_df = pd.concat([oof_df, test_df_], axis=0)\n",
    "        print(len(oof_df))\n",
    "\n",
    "\n",
    "train_df = pd.merge(train_df, oof_df[[\"user_id\", \"anime_id\", \"svd\"]], on=[\"user_id\", \"anime_id\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_svd = pd.read_csv('/workspace/input/atmaCup15_dataset/test.csv')\n",
    "test_svd['score'] = 0\n",
    "\n",
    "test_set = Dataset.load_from_df(test_svd, reader).build_full_trainset().build_testset()\n",
    "\n",
    "predictions = []\n",
    "for model in models:\n",
    "    pred = model.test(test_set)\n",
    "    pred_ = [pred.est for pred in pred]\n",
    "    predictions.append(pred_)\n",
    "\n",
    "pred_mean = np.mean(predictions, axis=0)\n",
    "test_svd[\"svd\"] = pred_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.merge(test_df, test_svd[[\"user_id\", \"anime_id\", \"svd\"]], on=[\"user_id\", \"anime_id\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013750 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32878\n",
      "[LightGBM] [Info] Number of data points in the train set: 109120, number of used features: 129\n",
      "[LightGBM] [Info] Start training from score 7.768759\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttraining's rmse: 1.16422\tvalid_1's rmse: 1.19141\n",
      "fold0 train RMSE: 1.164, seen val RMSE: 1.191, unseen val RMSE: 1.742\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014332 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32883\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 129\n",
      "[LightGBM] [Info] Start training from score 7.768725\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[63]\ttraining's rmse: 1.15287\tvalid_1's rmse: 1.1795\n",
      "fold1 train RMSE: 1.153, seen val RMSE: 1.179, unseen val RMSE: 2.470\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017014 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32879\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 129\n",
      "[LightGBM] [Info] Start training from score 7.768770\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's rmse: 1.15996\tvalid_1's rmse: 1.18536\n",
      "fold2 train RMSE: 1.160, seen val RMSE: 1.185, unseen val RMSE: 1.029\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014159 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32881\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 129\n",
      "[LightGBM] [Info] Start training from score 7.768798\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[47]\ttraining's rmse: 1.15936\tvalid_1's rmse: 1.18637\n",
      "fold3 train RMSE: 1.159, seen val RMSE: 1.185, unseen val RMSE: 3.948\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.021755 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32879\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 129\n",
      "[LightGBM] [Info] Start training from score 7.768798\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's rmse: 1.16366\tvalid_1's rmse: 1.18405\n",
      "fold4 train RMSE: 1.164, seen val RMSE: 1.184, unseen val RMSE: 2.041\n",
      "Total RMSE: 1.1853450343213374 | Seen RMSE: 1.1850622045319976 | Unseen RMSE: 2.546510738362592\n",
      "[Training and evaluation with LightGBM] done in 45 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Training and evaluation with LightGBM\"):\n",
    "    trained_df, tested_df, sub_df = train(train_df, test_df, submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/working/exp012_anime2vec/train_anime2vec.csv\n",
      "/workspace/working/exp012_anime2vec/test_anime2vec.csv\n",
      "/workspace/working/exp012_anime2vec/submission.csv\n"
     ]
    }
   ],
   "source": [
    "exp_name = \"exp012_anime2vec\"\n",
    "os.makedirs(os.path.join(\"/workspace\", \"working\", exp_name), exist_ok=True)\n",
    "\n",
    "\n",
    "train_path = os.path.join(\"/workspace\", \"working\", exp_name, 'train_anime2vec.csv')\n",
    "print(train_path)\n",
    "trained_df.to_csv(train_path, index=False)\n",
    "\n",
    "test_path = os.path.join(\"/workspace\", \"working\", exp_name,'test_anime2vec.csv')\n",
    "print(test_path)\n",
    "tested_df.to_csv(test_path, index=False)\n",
    "\n",
    "sub_path = os.path.join(\"/workspace\", \"working\", exp_name, 'submission.csv')\n",
    "print(sub_path)\n",
    "sub_df.to_csv(sub_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
