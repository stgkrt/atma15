{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback, log_summary\n",
    "import lightgbm as lgb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"exp000\"\n",
    "\n",
    "class configs:\n",
    "    EXP_CATEGORY = \"baseline\"\n",
    "    EXP_NAME = EXP_NAME\n",
    "    OUTPUT_DIR = os.path.join(\"/workspace\", \"working\", EXP_NAME)\n",
    "    \n",
    "    INPUT_DIR = os.path.join(\"/workspace\", \"input\", \"atmaCup15_dataset\")\n",
    "    # TRAIN_CSV = os.path.join(INPUT_DIR, \"train.csv\")\n",
    "    TRAIN_CSV = os.path.join(INPUT_DIR, \"train_stratifiedgroupkfold.csv\")\n",
    "    ANIME_CSV = os.path.join(INPUT_DIR, \"anime.csv\")\n",
    "    TEST_CSV = os.path.join(INPUT_DIR, \"test.csv\")\n",
    "    SAMPLE_SUB_CSV = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n",
    "    target_colname = \"score\"\n",
    "    \n",
    "    COMPETITION = \"atmaCup15\"\n",
    "    USER_NAME = \"taro\"\n",
    "    wandb_available = True\n",
    "    \n",
    "    # train\n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 200\n",
    "    verbose = 50\n",
    "    FOLDS = [0, 1, 2, 3, 4]\n",
    "\n",
    "if EXP_NAME is \"debug\":\n",
    "    os.makedirs(configs.OUTPUT_DIR)\n",
    "    configs.wandb_available = False\n",
    "    configs.num_boost_round = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    \"objective\": \"rmse\", \n",
    "    'metric': ['rmse', 'l2', 'l1', 'huber'],\n",
    "    \"n_estimators\": 10000, \n",
    "    \"learning_rate\": .1,\n",
    "    \"verbosity\": -1, \n",
    "    \"random_state\": 510,\n",
    "}\n",
    "configs.params = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, logger=None, format_str=\"{:.3f}[s]\", prefix=None, suffix=None, sep=\" \"):\n",
    "\n",
    "        if prefix: format_str = str(prefix) + sep + format_str\n",
    "        if suffix: format_str = format_str + sep + str(suffix)\n",
    "        self.format_str = format_str\n",
    "        self.logger = logger\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end is None:\n",
    "            return 0\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time()\n",
    "        out_str = self.format_str.format(self.duration)\n",
    "        if self.logger:\n",
    "            self.logger.info(out_str)\n",
    "        else:\n",
    "            print(out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"mean_squared_error の root (0.5乗)\"\"\"\n",
    "    return mean_squared_error(y_true, y_pred) ** .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_by_anime_id(left_df, right_df):\n",
    "    return pd.merge(left_df[\"anime_id\"], right_df, on=\"anime_id\", how=\"left\").drop(columns=[\"anime_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_numeric_feature(input_df: pd.DataFrame):\n",
    "    \"\"\"input_dfは train or test.csv のデータが入ってくることを想定しています.\"\"\"\n",
    "    \n",
    "    use_columns = [\n",
    "        \"members\", \n",
    "    ]\n",
    "    \n",
    "    return merge_by_anime_id(input_df, anime_df)[use_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをカウントエンコーディング\n",
    "def create_anime_type_count_encoding(input_df: pd.DataFrame):\n",
    "    count = anime_df[\"type\"].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        \"tyoe_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをone-hotエンコーディング\n",
    "def create_anime_type_one_hot_encoding(input_df: pd.DataFrame):\n",
    "    # 対象の列のユニーク集合を取る\n",
    "    target_colname = \"type\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    # ユニークな値ごとに列を作る\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        out_df[value] = is_value.astype(int)\n",
    "        \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    return merge_by_anime_id(input_df, out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上で定義した関数をまとめて実行\n",
    "def create_feature(input_df, config_):\n",
    "    # functions に特徴量作成関数を配列で定義しました.\n",
    "    # どの関数も同じ input / output のインターフェイスなので for で回せて嬉しいですね ;)\n",
    "    functions = [\n",
    "        create_anime_numeric_feature,\n",
    "        create_anime_type_count_encoding,\n",
    "        create_anime_type_one_hot_encoding,\n",
    "    ]\n",
    "    \n",
    "    out_df = pd.DataFrame()\n",
    "    func_name_list = []\n",
    "    for func in functions:\n",
    "        func_name = str(func.__name__)\n",
    "        func_name_list.append(func_name)\n",
    "        with Timer(prefix=f\"create {func_name}\"):\n",
    "            _df = func(input_df)\n",
    "        out_df = pd.concat([out_df, _df], axis=1)\n",
    "    \n",
    "    config_.preprocess_funcs = func_name_list\n",
    "    return out_df, config_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, fold):\n",
    "    save_path = os.path.join(configs.OUTPUT_DIR, f\"model_fold{fold}.pkl\")\n",
    "    # pickle.dump(model, save_path)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"SAVED: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgbm(df, configs):\n",
    "    \"\"\"lightGBM を CrossValidation の枠組みで学習を行なう function\"\"\"\n",
    "\n",
    "    models = []\n",
    "    evals_results_list = [] \n",
    "    n_records = len(df)\n",
    "    # training data の target と同じだけのゼロ配列を用意\n",
    "    oof_pred = np.zeros((n_records, ), dtype=np.float32)\n",
    "    target = []\n",
    "    for fold in configs.FOLDS: \n",
    "        # この部分が交差検証のところです。データセットを cv instance によって分割します\n",
    "        # training data を trian/valid に分割\n",
    "        train_df_ = df[df[\"fold\"] != fold].reset_index(drop=True)\n",
    "        valid_df_ = df[df[\"fold\"] == fold].reset_index(drop=True)\n",
    "        idx_valid = df[df[\"fold\"] == fold].index.values\n",
    "        \n",
    "        x_train = train_df_.drop(columns=[configs.target_colname, \"fold\"])\n",
    "        y_train = train_df_[configs.target_colname]\n",
    "        x_valid = valid_df_.drop(columns=[configs.target_colname, \"fold\"])\n",
    "        y_valid = valid_df_[configs.target_colname]\n",
    "        target.extend(y_valid)\n",
    "        \n",
    "        lgb_train = lgb.Dataset(x_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "        lgb_result = {}\n",
    "        with Timer(prefix=\"fit fold={} \".format(fold)):\n",
    "            if configs.wandb_available:\n",
    "                clf = lgb.train(configs.params,\n",
    "                                lgb_train,\n",
    "                                valid_sets=lgb_eval,  \n",
    "                                valid_names=[f\"validation_{fold}\"],\n",
    "                                num_boost_round=configs.num_boost_round,\n",
    "                                early_stopping_rounds=configs.early_stopping_rounds,\n",
    "                                evals_result=lgb_result,\n",
    "                                callbacks=[wandb_callback()])\n",
    "                evals_results_list.append(lgb_result)\n",
    "                log_summary(clf, save_model_checkpoint=False)\n",
    "\n",
    "            else:\n",
    "                clf = lgb.train(configs.params,\n",
    "                                lgb_train,\n",
    "                                valid_sets=[lgb_train, lgb_eval],  \n",
    "                                valid_names=[f\"validation_{fold}\"],\n",
    "                                num_boost_round=configs.num_boost_round,\n",
    "                                early_stopping_rounds=configs.early_stopping_rounds,\n",
    "                                evals_result=lgb_result,\n",
    "                                )\n",
    "                evals_results_list.append(lgb_result)\n",
    "        \n",
    "        # cv 内で validation data とされた x_valid で予測をして oof_pred に保存していく\n",
    "        # oof_pred は全部学習に使わなかったデータの予測結果になる → モデルの予測性能を見る指標として利用できる\n",
    "        pred_i = clf.predict(x_valid)\n",
    "        oof_pred[idx_valid] = pred_i\n",
    "        models.append(clf)\n",
    "        save_model(clf, fold)\n",
    "        score = root_mean_squared_error(y_valid, pred_i)\n",
    "        print(f\" - fold{fold} - {score:.4f}\")\n",
    "        if configs.wandb_available:\n",
    "            wandb.log({\"fold\": fold, \"rmse\": score})\n",
    "\n",
    "    score = root_mean_squared_error(target, oof_pred)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"FINISHI: Whole Score: {score:.4f}\")\n",
    "    return oof_pred, target, models, evals_results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = pd.read_csv(configs.ANIME_CSV)\n",
    "\n",
    "train_df = pd.read_csv(configs.TRAIN_CSV)\n",
    "test_df = pd.read_csv(configs.TEST_CSV)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess\n",
    "- CountEncoding\n",
    "- OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行して train / test 用の特徴量を作ります.\n",
    "\n",
    "with Timer(prefix=\"train...\"):\n",
    "    train_feat_df, configs = create_feature(train_df, configs)\n",
    "\n",
    "with Timer(prefix=\"test...\"):\n",
    "    test_feat_df, configs = create_feature(test_df, configs)\n",
    "\n",
    "# X = train_feat_df.values\n",
    "# print(train_feat_df.columns)\n",
    "# y = train_df[\"score\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.concat([train_df, train_feat_df], axis=1)\n",
    "input_df = input_df.drop([\"user_id\", \"anime_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if configs.wandb_available:\n",
    "   WANDB_CONFIG = {'competition': \"atma15\", '_wandb_kernel': \"taro\"}\n",
    "   os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "   # config_dict = dict(vars(configs))\n",
    "   wandb.init(project=WANDB_CONFIG[\"competition\"],\n",
    "               # config=config_dict,\n",
    "               group=configs.EXP_CATEGORY, \n",
    "               name=configs.EXP_NAME,\n",
    "               reinit=True,\n",
    "               save_code=True)\n",
    "   print(\"wandb initialized\")\n",
    "else:\n",
    "   print(\"wandb logging is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof, target, models, evals_results = fit_lgbm(input_df, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for evals in evals_results:\n",
    "    lgb.plot_metric(evals, metric=\"rmse\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_score = root_mean_squared_error(y_true=target, y_pred=oof)\n",
    "print(oof_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "sns.boxenplot(data=pd.DataFrame({ \"GroundTruth\": target, \"OutOfFold Prediction\": oof }), \n",
    "              x=\"GroundTruth\", y=\"OutOfFold Prediction\", ax=ax)\n",
    "\n",
    "ax.grid()\n",
    "ax.plot([0, 9], [1, 10], \"--\", c=\"black\", alpha=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(models):\n",
    "    lgb.plot_importance(model, figsize=(5,5),\n",
    "                        importance_type=\"gain\", max_num_features=25, \n",
    "                        xlabel=\"Feature Importance\", ylabel=\"Features\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k 個のモデルの予測を作成. shape = (5, N_test,).\n",
    "pred = np.array([model.predict(test_feat_df.values) for model in models])\n",
    "\n",
    "# k 個のモデルの予測値の平均 shape = (N_test,).\n",
    "pred = np.mean(pred, axis=0) # axis=0 なので shape の `k` が潰れる "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "vmax = .02\n",
    "bins = np.linspace(0, 10, 100)\n",
    "ax.hist(pred, bins=bins, density=True, alpha=.5, label=\"Test\")\n",
    "ax.hist(oof, bins=bins, density=True, alpha=.5, label=\"OutOfFold\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "# ax.set_title(\"テストと学習時の予測傾向差分\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save infer csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"score\": pred\n",
    "}).to_csv(os.path.join(configs.OUTPUT_DIR, \"submission.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if configs.wandb_available:\n",
    "    wandb.log({\"oof_score\": oof_score})\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
