{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle\n",
    "from glob import glob\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback, log_summary\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXP_NAME = \"exp006\"\n",
    "EXP_NAME = \"debug\"\n",
    "\n",
    "class configs:\n",
    "    EXP_CATEGORY = \"baseline\"\n",
    "    EXP_NAME = EXP_NAME\n",
    "    OUTPUT_DIR = os.path.join(\"/workspace\", \"working\", EXP_NAME)\n",
    "    \n",
    "    INPUT_DIR = os.path.join(\"/workspace\", \"input\", \"atmaCup15_dataset\")\n",
    "    # TRAIN_CSV = \"/workspace/working/svd_filtering_oof/train_addSVD.csv\"\n",
    "    TRAIN_CSV = \"/workspace/working/svdpp/train_addSVDpp.csv\"\n",
    "    ANIME_CSV = os.path.join(INPUT_DIR, \"anime.csv\")\n",
    "    # TEST_CSV = \"/workspace/working/svd_filtering_oof/test_addSVD.csv\"\n",
    "    TEST_CSV = \"/workspace/working/svdpp/test_addSVDpp.csv\"\n",
    "    SAMPLE_SUB_CSV = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n",
    "    target_colname = \"score\"\n",
    "    \n",
    "    COMPETITION = \"atmaCup15\"\n",
    "    USER_NAME = \"taro\"\n",
    "    wandb_available = True\n",
    "    \n",
    "    # train\n",
    "    num_boost_round = 1000\n",
    "    early_stopping_rounds = 200\n",
    "    verbose_eval = 100\n",
    "    FOLDS = [0, 1, 2, 3, 4]\n",
    "\n",
    "if EXP_NAME is \"debug\":\n",
    "    configs.wandb_available = False\n",
    "    configs.num_boost_round = 10\n",
    "else:\n",
    "    os.makedirs(configs.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    \"objective\": \"rmse\", \n",
    "    'metric': 'rmse',\n",
    "    \"n_estimators\": 10000, \n",
    "    \"learning_rate\": .01,\n",
    "    \"verbosity\": -1, \n",
    "    \"random_state\": 510,\n",
    "}\n",
    "configs.params = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, logger=None, format_str=\"{:.3f}[s]\", prefix=None, suffix=None, sep=\" \"):\n",
    "\n",
    "        if prefix: format_str = str(prefix) + sep + format_str\n",
    "        if suffix: format_str = format_str + sep + str(suffix)\n",
    "        self.format_str = format_str\n",
    "        self.logger = logger\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end is None:\n",
    "            return 0\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time()\n",
    "        out_str = self.format_str.format(self.duration)\n",
    "        if self.logger:\n",
    "            self.logger.info(out_str)\n",
    "        else:\n",
    "            print(out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"mean_squared_error の root (0.5乗)\"\"\"\n",
    "    return mean_squared_error(y_true, y_pred) ** .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_by_anime_id(left_df, right_df):\n",
    "    return pd.merge(left_df[\"anime_id\"], right_df, on=\"anime_id\", how=\"left\").drop(columns=[\"anime_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_by_user_id(left_df, right_df):\n",
    "    return pd.merge(left_df[\"user_id\"], right_df, on=\"user_id\", how=\"left\").drop(columns=[\"user_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_numeric_feature(input_df: pd.DataFrame):\n",
    "    \"\"\"input_dfは train or test.csv のデータが入ってくることを想定しています.\"\"\"\n",
    "    \n",
    "    use_columns = [\n",
    "        \"members\", \n",
    "    ]\n",
    "    \n",
    "    return merge_by_anime_id(input_df, anime_df)[use_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_genres_label_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"genres\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_le\": encoder.fit_transform(anime_df[target_col].fillna(\"nan\"))\n",
    "    })\n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_source_label_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"source\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_le\": encoder.fit_transform(anime_df[target_col].fillna(\"nan\"))\n",
    "    })\n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_animeid_label_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"anime_id\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_le\": encoder.fit_transform(anime_df[target_col].fillna(\"nan\"))\n",
    "    })\n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_userid_label_encoding(input_df: pd.DataFrame):\n",
    "    user_unique = set(train_df[\"user_id\"].unique()) | set(test_df[\"user_id\"].unique())\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(list(user_unique))\n",
    "    \n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"user_id\": input_df[\"user_id\"],\n",
    "        f\"user_le\": encoder.transform(input_df[\"user_id\"].fillna(\"nan\"))\n",
    "    })\n",
    "    \n",
    "    return merge_by_user_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをカウントエンコーディング\n",
    "def create_anime_type_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"type\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_studios_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"studios\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_producers_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"producers\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_animeid_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"anime_id\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_source_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"source\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupby encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをone-hotエンコーディング\n",
    "def create_anime_type_one_hot_encoding(input_df: pd.DataFrame):\n",
    "    # 対象の列のユニーク集合を取る\n",
    "    target_colname = \"type\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    # ユニークな値ごとに列を作る\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        if value == \"Unknown\":\n",
    "            out_df[\"Unknown_type\"] = is_value.astype(int)\n",
    "        else:\n",
    "            out_df[value] = is_value.astype(int)\n",
    "    \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    return merge_by_anime_id(input_df, out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをone-hotエンコーディング\n",
    "def create_anime_rating_one_hot_encoding(input_df: pd.DataFrame):\n",
    "    # 対象の列のユニーク集合を取る\n",
    "    target_colname = \"rating\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    # ユニークな値ごとに列を作る\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        if value == \"Unknown\":\n",
    "            out_df[\"Unknown_rate\"] = is_value.astype(int)\n",
    "        else:\n",
    "            out_df[value] = is_value.astype(int)\n",
    "    \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    return merge_by_anime_id(input_df, out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上で定義した関数をまとめて実行\n",
    "def create_feature(input_df, config_):\n",
    "    # functions に特徴量作成関数を配列で定義しました.\n",
    "    # どの関数も同じ input / output のインターフェイスなので for で回せて嬉しいですね ;)\n",
    "    functions = [\n",
    "        # create numeric\n",
    "        create_anime_numeric_feature,\n",
    "        # label encoding\n",
    "        create_anime_genres_label_encoding, \n",
    "        create_anime_source_label_encoding, \n",
    "        create_anime_animeid_label_encoding,\n",
    "        create_userid_label_encoding,\n",
    "        # count encoding\n",
    "        create_anime_type_count_encoding,\n",
    "        create_anime_studios_count_encoding,\n",
    "        create_anime_producers_count_encoding,\n",
    "        create_anime_animeid_count_encoding, \n",
    "        # one-hot encoding\n",
    "        create_anime_type_one_hot_encoding,\n",
    "        create_anime_rating_one_hot_encoding,\n",
    "        # group encoding\n",
    "    ]\n",
    "    \n",
    "    out_df = pd.DataFrame()\n",
    "    func_name_list = []\n",
    "    for func in functions:\n",
    "        func_name = str(func.__name__)\n",
    "        func_name_list.append(func_name)\n",
    "        with Timer(prefix=f\"create {func_name}\"):\n",
    "            _df = func(input_df)\n",
    "        out_df = pd.concat([out_df, _df], axis=1)\n",
    "    \n",
    "    config_.preprocess_funcs = func_name_list\n",
    "    return out_df, config_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert date in anime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_startdate(row):\n",
    "    if \" to \" in row.aired:\n",
    "        start_date, _ = row.aired.split(\" to \")\n",
    "        return pd.to_datetime(start_date)\n",
    "    elif row.aired == \"Unknown\":\n",
    "        return datetime.datetime.strptime(\"1900-01-01\", '%Y-%m-%d')\n",
    "    else:\n",
    "        start_date = row.aired\n",
    "        return pd.to_datetime(start_date)\n",
    "\n",
    "def convert_enddate(row):\n",
    "    if \" to \" in row.aired:\n",
    "        _, end_date = row.aired.split(\" to \")\n",
    "        if end_date == \"?\":\n",
    "            return datetime.datetime.strptime(\"2262-01-01\", '%Y-%m-%d')\n",
    "        else:\n",
    "            return pd.to_datetime(end_date)\n",
    "    elif row.aired == \"Unknown\":\n",
    "        return datetime.datetime.strptime(\"2262-01-01\", '%Y-%m-%d')\n",
    "    else:\n",
    "        end_date = row.aired\n",
    "        return pd.to_datetime(end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, fold):\n",
    "    save_path = os.path.join(configs.OUTPUT_DIR, f\"model_fold{fold}.pkl\")\n",
    "    # pickle.dump(model, save_path)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"SAVED: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgbm(df, configs):\n",
    "    \"\"\"lightGBM を CrossValidation の枠組みで学習を行なう function\"\"\"\n",
    "\n",
    "    models = []\n",
    "    evals_results_list = [] \n",
    "    n_records = len(df)\n",
    "    # training data の target と同じだけのゼロ配列を用意\n",
    "    oof_pred = np.zeros((n_records, ), dtype=np.float32)\n",
    "    target = []\n",
    "    for fold in configs.FOLDS: \n",
    "        # この部分が交差検証のところです。データセットを cv instance によって分割します\n",
    "        # training data を trian/valid に分割\n",
    "        train_df_ = df[df[\"fold\"] != fold].reset_index(drop=True)\n",
    "        valid_df_ = df[df[\"fold\"] == fold].reset_index(drop=True)\n",
    "        idx_valid = df[df[\"fold\"] == fold].index.values\n",
    "        \n",
    "        x_train = train_df_.drop(columns=[configs.target_colname, \"fold\"])\n",
    "        # print(x_train.columns)\n",
    "        y_train = train_df_[configs.target_colname]\n",
    "        x_valid = valid_df_.drop(columns=[configs.target_colname, \"fold\"])\n",
    "        y_valid = valid_df_[configs.target_colname]\n",
    "        target.extend(y_valid)\n",
    "        \n",
    "        lgb_train = lgb.Dataset(x_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "        lgb_result = {}\n",
    "        with Timer(prefix=\"fit fold={} \".format(fold)):\n",
    "            if configs.wandb_available:\n",
    "                clf = lgb.train(configs.params,\n",
    "                                lgb_train,\n",
    "                                valid_sets=[lgb_train, lgb_eval],  \n",
    "                                valid_names=[f\"validation_{fold}\"],\n",
    "                                num_boost_round=configs.num_boost_round,\n",
    "                                early_stopping_rounds=configs.early_stopping_rounds,\n",
    "                                evals_result=lgb_result,\n",
    "                                verbose_eval=configs.verbose_eval,\n",
    "                                callbacks=[wandb_callback()])\n",
    "                evals_results_list.append(lgb_result)\n",
    "                log_summary(clf, save_model_checkpoint=False)\n",
    "\n",
    "            else:\n",
    "                clf = lgb.train(configs.params,\n",
    "                                lgb_train,\n",
    "                                valid_sets=[lgb_train, lgb_eval],  \n",
    "                                valid_names=[f\"validation_{fold}\"],\n",
    "                                num_boost_round=configs.num_boost_round,\n",
    "                                early_stopping_rounds=configs.early_stopping_rounds,\n",
    "                                evals_result=lgb_result,\n",
    "                                verbose_eval=configs.verbose_eval,\n",
    "                                )\n",
    "                evals_results_list.append(lgb_result)\n",
    "        \n",
    "        # cv 内で validation data とされた x_valid で予測をして oof_pred に保存していく\n",
    "        # oof_pred は全部学習に使わなかったデータの予測結果になる → モデルの予測性能を見る指標として利用できる\n",
    "        pred_i = clf.predict(x_valid)\n",
    "        oof_pred[idx_valid] = pred_i\n",
    "        models.append(clf)\n",
    "        save_model(clf, fold)\n",
    "        score = root_mean_squared_error(y_valid, pred_i)\n",
    "        print(f\" - fold{fold} - {score:.4f}\")\n",
    "        if configs.wandb_available:\n",
    "            wandb.log({\"fold\": fold, \"rmse\": score})\n",
    "\n",
    "    score = root_mean_squared_error(target, oof_pred)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"FINISHI: Whole Score: {score:.4f}\")\n",
    "    return oof_pred, target, models, evals_results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = pd.read_csv(configs.ANIME_CSV)\n",
    "\n",
    "train_df = pd.read_csv(configs.TRAIN_CSV)\n",
    "test_df = pd.read_csv(configs.TEST_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df[\"start_date\"] = anime_df.apply(convert_startdate, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df[\"end_date\"] = anime_df.apply(convert_enddate, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess\n",
    "- CountEncoding\n",
    "- OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create create_anime_numeric_feature 0.061[s]\n",
      "create create_anime_genres_label_encoding 0.026[s]\n",
      "create create_anime_source_label_encoding 0.012[s]\n",
      "create create_anime_animeid_label_encoding 0.016[s]\n",
      "create create_userid_label_encoding 0.801[s]\n",
      "create create_anime_type_count_encoding 0.019[s]\n",
      "create create_anime_studios_count_encoding 0.013[s]\n",
      "create create_anime_producers_count_encoding 0.011[s]\n",
      "create create_anime_animeid_count_encoding 0.012[s]\n",
      "create create_anime_type_one_hot_encoding 0.016[s]\n",
      "create create_anime_rating_one_hot_encoding 0.015[s]\n",
      "train... 8.727[s]\n",
      "create create_anime_numeric_feature 0.053[s]\n",
      "create create_anime_genres_label_encoding 0.016[s]\n",
      "create create_anime_source_label_encoding 0.011[s]\n",
      "create create_anime_animeid_label_encoding 0.012[s]\n",
      "create create_userid_label_encoding 0.585[s]\n",
      "create create_anime_type_count_encoding 0.017[s]\n",
      "create create_anime_studios_count_encoding 0.011[s]\n",
      "create create_anime_producers_count_encoding 0.013[s]\n",
      "create create_anime_animeid_count_encoding 0.011[s]\n",
      "create create_anime_type_one_hot_encoding 0.014[s]\n",
      "create create_anime_rating_one_hot_encoding 0.014[s]\n",
      "test... 6.729[s]\n"
     ]
    }
   ],
   "source": [
    "# 実行して train / test 用の特徴量を作ります.\n",
    "\n",
    "with Timer(prefix=\"train...\"):\n",
    "    train_feat_df, configs = create_feature(train_df, configs)\n",
    "\n",
    "with Timer(prefix=\"test...\"):\n",
    "    test_feat_df, configs = create_feature(test_df, configs)\n",
    "\n",
    "# X = train_feat_df.values\n",
    "# print(train_feat_df.columns)\n",
    "# y = train_df[\"score\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.concat([train_df, train_feat_df], axis=1)\n",
    "input_df = input_df.drop([\"user_id\", \"anime_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['score', 'fold', 'svd', 'members', 'genres_le', 'source_le',\n",
       "       'anime_id_le', 'user_le', 'type_count', 'studios_count',\n",
       "       'producers_count', 'anime_id_count', 'TV', 'Special', 'Movie',\n",
       "       'Unknown_type', 'ONA', 'OVA', 'Music', 'PG-13 - Teens 13 or older',\n",
       "       'R+ - Mild Nudity', 'R - 17+ (violence & profanity)', 'G - All Ages',\n",
       "       'PG - Children', 'Rx - Hentai', 'Unknown_rate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb logging is not available\n"
     ]
    }
   ],
   "source": [
    "if configs.wandb_available:\n",
    "   WANDB_CONFIG = {'competition': \"atma15\", '_wandb_kernel': \"taro\"}\n",
    "   os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "   # config_dict = dict(vars(configs))\n",
    "   wandb.init(project=WANDB_CONFIG[\"competition\"],\n",
    "               # config=config_dict,\n",
    "               group=configs.EXP_CATEGORY, \n",
    "               name=configs.EXP_NAME,\n",
    "               reinit=True,\n",
    "               save_code=True)\n",
    "   print(\"wandb initialized\")\n",
    "else:\n",
    "   print(\"wandb logging is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalidation_0's rmse: 0.24071\tvalid_1's rmse: 3.08805\n",
      "[200]\tvalidation_0's rmse: 0.12282\tvalid_1's rmse: 1.58196\n",
      "[300]\tvalidation_0's rmse: 0.0964951\tvalid_1's rmse: 1.24973\n",
      "[400]\tvalidation_0's rmse: 0.0922364\tvalid_1's rmse: 1.19871\n",
      "[500]\tvalidation_0's rmse: 0.0915123\tvalid_1's rmse: 1.19222\n",
      "[600]\tvalidation_0's rmse: 0.0912925\tvalid_1's rmse: 1.19274\n",
      "[700]\tvalidation_0's rmse: 0.0911528\tvalid_1's rmse: 1.19288\n",
      "Early stopping, best iteration is:\n",
      "[510]\tvalidation_0's rmse: 0.091482\tvalid_1's rmse: 1.19207\n",
      "fit fold=0  79.076[s]\n",
      "SAVED: /workspace/working/debug/model_fold0.pkl\n",
      " - fold0 - 1.1921\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalidation_1's rmse: 0.240734\tvalid_1's rmse: 3.08927\n",
      "[200]\tvalidation_1's rmse: 0.122885\tvalid_1's rmse: 1.58197\n",
      "[300]\tvalidation_1's rmse: 0.0965781\tvalid_1's rmse: 1.24849\n",
      "[400]\tvalidation_1's rmse: 0.0923301\tvalid_1's rmse: 1.19654\n",
      "[500]\tvalidation_1's rmse: 0.0915873\tvalid_1's rmse: 1.18944\n",
      "[600]\tvalidation_1's rmse: 0.0913606\tvalid_1's rmse: 1.1886\n",
      "[700]\tvalidation_1's rmse: 0.0912262\tvalid_1's rmse: 1.18861\n",
      "[800]\tvalidation_1's rmse: 0.0911053\tvalid_1's rmse: 1.1887\n",
      "Early stopping, best iteration is:\n",
      "[643]\tvalidation_1's rmse: 0.0912977\tvalid_1's rmse: 1.18854\n",
      "fit fold=1  101.949[s]\n",
      "SAVED: /workspace/working/debug/model_fold1.pkl\n",
      " - fold1 - 1.1885\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalidation_2's rmse: 0.240764\tvalid_1's rmse: 3.08525\n",
      "[200]\tvalidation_2's rmse: 0.122946\tvalid_1's rmse: 1.57636\n",
      "[300]\tvalidation_2's rmse: 0.0966584\tvalid_1's rmse: 1.24298\n",
      "[400]\tvalidation_2's rmse: 0.0924148\tvalid_1's rmse: 1.1916\n",
      "[500]\tvalidation_2's rmse: 0.0916881\tvalid_1's rmse: 1.18504\n",
      "[600]\tvalidation_2's rmse: 0.0914681\tvalid_1's rmse: 1.18461\n",
      "[700]\tvalidation_2's rmse: 0.091326\tvalid_1's rmse: 1.18486\n",
      "Early stopping, best iteration is:\n",
      "[585]\tvalidation_2's rmse: 0.0914902\tvalid_1's rmse: 1.18459\n",
      "fit fold=2  98.712[s]\n",
      "SAVED: /workspace/working/debug/model_fold2.pkl\n",
      " - fold2 - 1.1846\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalidation_3's rmse: 0.24073\tvalid_1's rmse: 3.09423\n",
      "[200]\tvalidation_3's rmse: 0.122867\tvalid_1's rmse: 1.58749\n",
      "[300]\tvalidation_3's rmse: 0.0965558\tvalid_1's rmse: 1.25139\n",
      "[400]\tvalidation_3's rmse: 0.0923066\tvalid_1's rmse: 1.19831\n",
      "[500]\tvalidation_3's rmse: 0.0915814\tvalid_1's rmse: 1.19087\n",
      "[600]\tvalidation_3's rmse: 0.0913633\tvalid_1's rmse: 1.18991\n",
      "[700]\tvalidation_3's rmse: 0.0912391\tvalid_1's rmse: 1.19003\n",
      "[800]\tvalidation_3's rmse: 0.0911345\tvalid_1's rmse: 1.19018\n",
      "Early stopping, best iteration is:\n",
      "[618]\tvalidation_3's rmse: 0.0913354\tvalid_1's rmse: 1.18989\n",
      "fit fold=3  98.775[s]\n",
      "SAVED: /workspace/working/debug/model_fold3.pkl\n",
      " - fold3 - 1.1899\n",
      "Training until validation scores don't improve for 200 rounds\n",
      "[100]\tvalidation_4's rmse: 0.240735\tvalid_1's rmse: 3.08699\n",
      "[200]\tvalidation_4's rmse: 0.122876\tvalid_1's rmse: 1.58002\n",
      "[300]\tvalidation_4's rmse: 0.0965627\tvalid_1's rmse: 1.24782\n",
      "[400]\tvalidation_4's rmse: 0.092311\tvalid_1's rmse: 1.19702\n",
      "[500]\tvalidation_4's rmse: 0.0915843\tvalid_1's rmse: 1.19033\n",
      "[600]\tvalidation_4's rmse: 0.0913714\tvalid_1's rmse: 1.18982\n",
      "[700]\tvalidation_4's rmse: 0.0912436\tvalid_1's rmse: 1.19\n",
      "Early stopping, best iteration is:\n",
      "[577]\tvalidation_4's rmse: 0.0914076\tvalid_1's rmse: 1.1898\n",
      "fit fold=4  86.485[s]\n",
      "SAVED: /workspace/working/debug/model_fold4.pkl\n",
      " - fold4 - 1.1898\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [136401, 18070691]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m oof, target, models, evals_results \u001b[39m=\u001b[39m fit_lgbm(input_df, configs)\n",
      "Cell \u001b[0;32mIn[23], line 65\u001b[0m, in \u001b[0;36mfit_lgbm\u001b[0;34m(df, configs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[39mif\u001b[39;00m configs\u001b[39m.\u001b[39mwandb_available:\n\u001b[1;32m     63\u001b[0m         wandb\u001b[39m.\u001b[39mlog({\u001b[39m\"\u001b[39m\u001b[39mfold\u001b[39m\u001b[39m\"\u001b[39m: fold, \u001b[39m\"\u001b[39m\u001b[39mrmse\u001b[39m\u001b[39m\"\u001b[39m: score})\n\u001b[0;32m---> 65\u001b[0m score \u001b[39m=\u001b[39m root_mean_squared_error(target, oof_pred)\n\u001b[1;32m     67\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m50\u001b[39m)\n\u001b[1;32m     68\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFINISHI: Whole Score: \u001b[39m\u001b[39m{\u001b[39;00mscore\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m, in \u001b[0;36mroot_mean_squared_error\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mroot_mean_squared_error\u001b[39m(y_true, y_pred):\n\u001b[1;32m      2\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"mean_squared_error の root (0.5乗)\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     \u001b[39mreturn\u001b[39;00m mean_squared_error(y_true, y_pred) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m.5\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:442\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean_squared_error\u001b[39m(\n\u001b[1;32m    383\u001b[0m     y_true, y_pred, \u001b[39m*\u001b[39m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, multioutput\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muniform_average\u001b[39m\u001b[39m\"\u001b[39m, squared\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    384\u001b[0m ):\n\u001b[1;32m    385\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \n\u001b[1;32m    387\u001b[0m \u001b[39m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m    0.825...\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[39m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    443\u001b[0m         y_true, y_pred, multioutput\n\u001b[1;32m    444\u001b[0m     )\n\u001b[1;32m    445\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    446\u001b[0m     output_errors \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39maverage((y_true \u001b[39m-\u001b[39m y_pred) \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, weights\u001b[39m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_regression.py:100\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mnumeric\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     67\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \n\u001b[1;32m     69\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m        correct keyword.\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[1;32m    101\u001b[0m     y_true \u001b[39m=\u001b[39m check_array(y_true, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    102\u001b[0m     y_pred \u001b[39m=\u001b[39m check_array(y_pred, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/utils/validation.py:397\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    395\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    398\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    399\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    400\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [136401, 18070691]"
     ]
    }
   ],
   "source": [
    "oof, target, models, evals_results = fit_lgbm(input_df, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for evals in evals_results:\n",
    "    lgb.plot_metric(evals, metric=\"rmse\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_score = root_mean_squared_error(y_true=target, y_pred=oof)\n",
    "print(oof_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "sns.boxenplot(data=pd.DataFrame({ \"GroundTruth\": target, \"OutOfFold Prediction\": oof }), \n",
    "              x=\"GroundTruth\", y=\"OutOfFold Prediction\", ax=ax)\n",
    "\n",
    "ax.grid()\n",
    "ax.plot([0, 9], [1, 10], \"--\", c=\"black\", alpha=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(models):\n",
    "    lgb.plot_importance(model, figsize=(5,5),\n",
    "                        importance_type=\"gain\", max_num_features=25, \n",
    "                        xlabel=\"Feature Importance\", ylabel=\"Features\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['svd', 'members', 'tyoe_count', 'TV', 'Special', 'Movie', 'Unknown','ONA', 'OVA', 'Music']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = [col for col in input_df.columns if col not in [\"fold\", \"score\"]]\n",
    "print(input_columns)\n",
    "print(len(input_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([test_df, test_feat_df], axis=1)\n",
    "test_df = test_df[input_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k 個のモデルの予測を作成. shape = (5, N_test,).\n",
    "pred = np.array([model.predict(test_df.values) for model in models])\n",
    "\n",
    "\n",
    "# k 個のモデルの予測値の平均 shape = (N_test,).\n",
    "pred = np.mean(pred, axis=0) # axis=0 なので shape の `k` が潰れる "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "vmax = .02\n",
    "bins = np.linspace(0, 10, 100)\n",
    "ax.hist(pred, bins=bins, density=True, alpha=.5, label=\"Test\")\n",
    "ax.hist(oof, bins=bins, density=True, alpha=.5, label=\"OutOfFold\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "# ax.set_title(\"テストと学習時の予測傾向差分\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save infer csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"score\": pred\n",
    "}).to_csv(os.path.join(configs.OUTPUT_DIR, \"submission.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if configs.wandb_available:\n",
    "    wandb.log({\"oof_score\": oof_score})\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
