{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime2Vec(Explicit/Implicit Feedback) + LightGBM\n",
    "\n",
    "[前回のnotebook](https://www.guruguru.science/competitions/21/discussions/57f5ea4e-69ad-439d-bbe3-887240cf5cf2/)ではレーティングの情報を活用する場合とそうでない場合を比較しましたが、今回はそれらを組み合わせることを考えます。\n",
    "\n",
    "[こちらのdiscussion](https://www.guruguru.science/competitions/21/discussions/d0e9e563-0910-46b9-a562-441b9c2bb843/)で紹介されているように、推薦システムにおけるユーザーからのフィードバックはExplicit FeedbackとImplicit Feedbackに分けることができます。  \n",
    "今回のAnime2Vecにおいて考えると、ユーザーレーティングを使用した場合はExplicit Feedbackをモデル化し、使用しない場合はある意味でImplicit Feedbackをモデル化していると考えることができます。  \n",
    "\n",
    "また、今回は`test.csv`が丸ごと与えられ、新規ユーザー（コールドユーザー）に対してもまとまった視聴情報、つまりImplicit Feedbackを得られているやや特殊な状況とも言えます。  \n",
    "折角なのでこれを活用する方法を考えたいところです。  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
    "from gensim.models import word2vec\n",
    "\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 0\n",
    "\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "def seed_everything(seed: int):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vecによる特徴量エンジニアリング(Explicit Feedback)\n",
    "\n",
    "こちらは[前回のnotebook](https://www.guruguru.science/competitions/21/discussions/57f5ea4e-69ad-439d-bbe3-887240cf5cf2/)と同様に、ユーザーのレーティングを考慮し、視聴回数分アニメを追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_w2v_features_with_score(train_df, val_df, test_df=None):\n",
    "    anime_ids = train_df['anime_id'].unique().tolist()\n",
    "    user_anime_list_dict = {user_id: anime_ids.tolist() for user_id, anime_ids in train_df.groupby('user_id')['anime_id']}\n",
    "\n",
    "    # スコアを考慮する場合\n",
    "    # 今回は1～10のレーティングなので、スコアが5のアニメは5回、スコアが10のアニメは10回、タイトルをリストに追加する\n",
    "    title_sentence_list = []\n",
    "    for user_id, user_df in train_df.groupby('user_id'):\n",
    "        user_title_sentence_list = []\n",
    "        for anime_id, anime_score in user_df[['anime_id', 'score']].values:\n",
    "            for i in range(anime_score):\n",
    "                user_title_sentence_list.append(anime_id)\n",
    "        title_sentence_list.append(user_title_sentence_list)\n",
    "\n",
    "    # ユーザごとにshuffleしたリストを作成\n",
    "    shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in title_sentence_list]  ## <= 変更点\n",
    "\n",
    "    # 元のリストとshuffleしたリストを合わせる\n",
    "    train_sentence_list = title_sentence_list + shuffled_sentence_list\n",
    "\n",
    "    # word2vecのパラメータ\n",
    "    vector_size = 64\n",
    "    w2v_params = {\n",
    "        \"vector_size\": vector_size,  ## <= 変更点\n",
    "        \"seed\": SEED,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 1\n",
    "    }\n",
    "\n",
    "    # word2vecのモデル学習\n",
    "    model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n",
    "\n",
    "    # ユーザーごとの特徴ベクトルと対応するユーザーID\n",
    "    user_factors = {user_id: np.mean([model.wv[anime_id] for anime_id in user_anime_list], axis=0) for user_id, user_anime_list in user_anime_list_dict.items()}\n",
    "\n",
    "    # アイテムごとの特徴ベクトルと対応するアイテムID\n",
    "    item_factors = {aid: model.wv[aid] for aid in anime_ids}\n",
    "\n",
    "    # データフレームを作成\n",
    "    user_factors_df = pd.DataFrame(user_factors).T.reset_index().rename(columns={\"index\": \"user_id\"})\n",
    "    item_factors_df = pd.DataFrame(item_factors).T.reset_index().rename(columns={\"index\": \"anime_id\"})\n",
    "\n",
    "    # データフレームのカラム名をリネーム\n",
    "    user_factors_df.columns = [\"user_id\"] + [f\"user_factor_{i}\" for i in range(vector_size)]\n",
    "    item_factors_df.columns = [\"anime_id\"] + [f\"item_factor_{i}\" for i in range(vector_size)]\n",
    "\n",
    "    train_df = train_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "    train_df = train_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n",
    "\n",
    "    val_df = val_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "    val_df = val_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n",
    "\n",
    "    if test_df is not None:\n",
    "        test_df = test_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "        test_df = test_df.merge(item_factors_df, on=\"anime_id\", how=\"left\")\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    return train_df, val_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vecによる特徴量エンジニアリング(Implicit Feedback)\n",
    "\n",
    "今回は`test.csv`が丸ごと与えられ、新規ユーザーに対しても視聴情報が得られる状況なのでこれを活用します。  \n",
    "とは言っても、単にtrain/testを先に結合してからスコアの情報を使用せずにWord2Vecの学習を行うだけです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_w2v_features_without_score(train_test_df):\n",
    "    \n",
    "    anime_ids = train_test_df['anime_id'].unique().tolist()\n",
    "    user_anime_list_dict = {user_id: anime_ids.tolist() for user_id, anime_ids in train_test_df.groupby('user_id')['anime_id']}\n",
    "\n",
    "    title_sentence_list = train_test_df.groupby('user_id')['anime_id'].apply(list).tolist()\n",
    "\n",
    "    # ユーザごとにshuffleしたリストを作成\n",
    "    shuffled_sentence_list = [random.sample(sentence, len(sentence)) for sentence in title_sentence_list]  ## <= 変更点\n",
    "\n",
    "    # 元のリストとshuffleしたリストを合わせる\n",
    "    train_sentence_list = title_sentence_list + shuffled_sentence_list\n",
    "\n",
    "    # word2vecのパラメータ\n",
    "    vector_size = 64\n",
    "    w2v_params = {\n",
    "        \"vector_size\": vector_size,  ## <= 変更点\n",
    "        \"seed\": SEED,\n",
    "        \"min_count\": 1,\n",
    "        \"workers\": 1\n",
    "    }\n",
    "\n",
    "    # word2vecのモデル学習\n",
    "    model = word2vec.Word2Vec(train_sentence_list, **w2v_params)\n",
    "\n",
    "    # ユーザーごとの特徴ベクトルと対応するユーザーID\n",
    "    user_factors = {user_id: np.mean([model.wv[anime_id] for anime_id in user_anime_list], axis=0) for user_id, user_anime_list in user_anime_list_dict.items()}\n",
    "\n",
    "    # アイテムごとの特徴ベクトルと対応するアイテムID\n",
    "    item_factors = {aid: model.wv[aid] for aid in anime_ids}\n",
    "\n",
    "    # データフレームを作成\n",
    "    user_factors_df = pd.DataFrame(user_factors).T.reset_index().rename(columns={\"index\": \"user_id\"})\n",
    "    item_factors_df = pd.DataFrame(item_factors).T.reset_index().rename(columns={\"index\": \"anime_id\"})\n",
    "\n",
    "    # データフレームのカラム名をリネーム\n",
    "    user_factors_df.columns = [\"user_id\"] + [f\"wo_score_user_factor_{i}\" for i in range(vector_size)]\n",
    "    item_factors_df.columns = [\"anime_id\"] + [f\"wo_score_item_factor_{i}\" for i in range(vector_size)]\n",
    "\n",
    "    train_test_df = train_test_df.merge(user_factors_df, on=\"user_id\", how=\"left\")\n",
    "\n",
    "    return train_test_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習に便利な関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_df = pd.read_csv('/workspace/input/atmaCup15_dataset/train.csv')\n",
    "    test_df = pd.read_csv('/workspace/input/atmaCup15_dataset/test.csv')\n",
    "    test_df['score'] = 0 # dummy\n",
    "\n",
    "    # Initialize submission file\n",
    "    submission_df = pd.read_csv('/workspace/input/atmaCup15_dataset/sample_submission.csv')\n",
    "    submission_df['score'] = 0\n",
    "    return train_df, test_df, submission_df\n",
    "\n",
    "def stratified_and_group_kfold_split(train_df):\n",
    "    # https://www.guruguru.science/competitions/21/discussions/45ffc8a1-e37c-4b95-aac4-c4e338aa6a9b/\n",
    "    \n",
    "    # 20%のユーザを抽出\n",
    "    n_user = train_df[\"user_id\"].nunique()\n",
    "    unseen_users = random.sample(sorted(train_df[\"user_id\"].unique()), k=n_user // 5)\n",
    "    train_df[\"unseen_user\"] = train_df[\"user_id\"].isin(unseen_users)\n",
    "    unseen_df = train_df[train_df[\"unseen_user\"]].reset_index(drop=True)\n",
    "    train_df = train_df[~train_df[\"unseen_user\"]].reset_index(drop=True)\n",
    "\n",
    "    # train_dfの80%をStratifiedKFoldで分割\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    for fold_id, (_, valid_idx) in enumerate(skf.split(train_df, train_df[\"user_id\"])):\n",
    "        train_df.loc[valid_idx, \"fold\"] = fold_id\n",
    "\n",
    "    # 20%をGroupKFoldで分割\n",
    "    gkf = GroupKFold(n_splits=5)\n",
    "    unseen_df[\"fold\"] = -1\n",
    "    for fold_id, (_, valid_idx) in enumerate(gkf.split(unseen_df, unseen_df[\"user_id\"], unseen_df[\"user_id\"])):\n",
    "        unseen_df.loc[valid_idx, \"fold\"] = fold_id\n",
    "\n",
    "    # concat\n",
    "    train_df = pd.concat([train_df, unseen_df], axis=0).reset_index(drop=True)\n",
    "    train_df.drop(columns=[\"unseen_user\"], inplace=True)\n",
    "    return train_df\n",
    "\n",
    "\n",
    "def train(train_df, original_test_df, submission_df):\n",
    "    train_df['oof'] = 0\n",
    "    train_df['seen'] = False\n",
    "\n",
    "    for fold in range(5):\n",
    "        # Prepare the train and validation data\n",
    "        trn_df = train_df[train_df['fold'] != fold].copy()\n",
    "        val_df = train_df[train_df['fold'] == fold].copy()\n",
    "\n",
    "        trn_df, val_df, test_df = add_w2v_features_with_score(trn_df, val_df, original_test_df.copy())\n",
    "        \n",
    "        # Define the features and the target\n",
    "        unused_cols = ['user_id', 'anime_id', 'score', 'fold', 'oof', 'seen']\n",
    "        feature_cols = [col for col in trn_df.columns if col not in unused_cols]\n",
    "        target_col = 'score'\n",
    "\n",
    "        # Prepare the LightGBM datasets\n",
    "        lgb_train = lgb.Dataset(trn_df[feature_cols], trn_df[target_col])\n",
    "        lgb_val = lgb.Dataset(val_df[feature_cols], val_df[target_col])\n",
    "\n",
    "        params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'learning_rate': 0.1,\n",
    "            # 'reg_lambda': 1.0\n",
    "        }\n",
    "\n",
    "        # Train the model\n",
    "        callbacks = [\n",
    "            lgb.early_stopping(stopping_rounds=100),\n",
    "            lgb.log_evaluation(period=100)\n",
    "        ]\n",
    "        model_lgb = lgb.train(params, lgb_train, valid_sets=[\n",
    "                              lgb_train, lgb_val], callbacks=callbacks, num_boost_round=10000)\n",
    "\n",
    "        # Predict\n",
    "        trn_df['preds'] = model_lgb.predict(trn_df[feature_cols], num_iteration=model_lgb.best_iteration)\n",
    "        val_df['preds'] = model_lgb.predict(val_df[feature_cols], num_iteration=model_lgb.best_iteration)\n",
    "        test_preds = model_lgb.predict(test_df[feature_cols], num_iteration=model_lgb.best_iteration)\n",
    "\n",
    "        train_users = trn_df['user_id'].unique()\n",
    "        is_seen = val_df['user_id'].isin(train_users)\n",
    "        seen_val = val_df[is_seen]\n",
    "        unseen_val = val_df[~is_seen]\n",
    "\n",
    "        # Evaluate the model\n",
    "        train_score = np.sqrt(mean_squared_error(trn_df['score'], trn_df['preds']))\n",
    "        seen_val_score = np.sqrt(mean_squared_error(seen_val['score'], seen_val['preds']))\n",
    "        unseen_val_score = np.sqrt(mean_squared_error(unseen_val['score'], unseen_val['preds']))\n",
    "        print(f'fold{fold} train RMSE: {train_score:.3f}, seen val RMSE: {seen_val_score:.3f}, unseen val RMSE: {unseen_val_score:.3f}')\n",
    "        \n",
    "        submission_df['score'] += test_preds / 5\n",
    "\n",
    "        train_df.loc[train_df['fold'] == fold, 'oof'] = val_df['preds'].values\n",
    "        train_df.loc[train_df['fold'] == fold, 'seen'] = is_seen.values\n",
    "\n",
    "    total_score = np.sqrt(mean_squared_error(train_df['score'], train_df['oof']))\n",
    "    seen_score = np.sqrt(mean_squared_error(train_df[train_df['seen']]['score'], train_df[train_df['seen']]['oof']))\n",
    "    unseen_score = np.sqrt(mean_squared_error(train_df[~train_df['seen']]['score'], train_df[~train_df['seen']]['oof']))\n",
    "    print(f\"Total RMSE: {total_score} | Seen RMSE: {seen_score} | Unseen RMSE: {unseen_score}\")\n",
    "\n",
    "    # train_df.to_csv(os.path.join(\"workspace\", \"working\", \"anime2vec\",'train_anime2vec.csv'), index=False)\n",
    "    # submission_df.to_csv(os.path.join(\"workspace\", \"working\", \"anime2vec\",'submission.csv'), index=False)o\n",
    "    return train_df, test_df, submission_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Load the data] done in 0 s\n",
      "[Stratified & Group split] done in 0 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[add_w2v_features_without_score] done in 2 s\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025097 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 48946\n",
      "[LightGBM] [Info] Number of data points in the train set: 109120, number of used features: 192\n",
      "[LightGBM] [Info] Start training from score 7.769401\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.19326\tvalid_1's rmse: 1.25371\n",
      "[200]\ttraining's rmse: 1.13219\tvalid_1's rmse: 1.23215\n",
      "[300]\ttraining's rmse: 1.09239\tvalid_1's rmse: 1.22745\n",
      "[400]\ttraining's rmse: 1.05656\tvalid_1's rmse: 1.22442\n",
      "[500]\ttraining's rmse: 1.02384\tvalid_1's rmse: 1.22187\n",
      "[600]\ttraining's rmse: 0.99401\tvalid_1's rmse: 1.22066\n",
      "[700]\ttraining's rmse: 0.966113\tvalid_1's rmse: 1.21998\n",
      "[800]\ttraining's rmse: 0.939603\tvalid_1's rmse: 1.2194\n",
      "[900]\ttraining's rmse: 0.915844\tvalid_1's rmse: 1.21881\n",
      "[1000]\ttraining's rmse: 0.892012\tvalid_1's rmse: 1.2189\n",
      "Early stopping, best iteration is:\n",
      "[901]\ttraining's rmse: 0.915595\tvalid_1's rmse: 1.21879\n",
      "fold0 train RMSE: 0.916, seen val RMSE: 1.179, unseen val RMSE: 1.374\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.022066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 48941\n",
      "[LightGBM] [Info] Number of data points in the train set: 109120, number of used features: 192\n",
      "[LightGBM] [Info] Start training from score 7.770684\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.19249\tvalid_1's rmse: 1.25971\n",
      "[200]\ttraining's rmse: 1.13035\tvalid_1's rmse: 1.23776\n",
      "[300]\ttraining's rmse: 1.09089\tvalid_1's rmse: 1.23205\n",
      "[400]\ttraining's rmse: 1.05597\tvalid_1's rmse: 1.22911\n",
      "[500]\ttraining's rmse: 1.02257\tvalid_1's rmse: 1.22731\n",
      "[600]\ttraining's rmse: 0.993155\tvalid_1's rmse: 1.2258\n",
      "[700]\ttraining's rmse: 0.96601\tvalid_1's rmse: 1.22435\n",
      "[800]\ttraining's rmse: 0.940166\tvalid_1's rmse: 1.22363\n",
      "[900]\ttraining's rmse: 0.915262\tvalid_1's rmse: 1.22369\n",
      "Early stopping, best iteration is:\n",
      "[816]\ttraining's rmse: 0.936116\tvalid_1's rmse: 1.22335\n",
      "fold1 train RMSE: 0.936, seen val RMSE: 1.180, unseen val RMSE: 1.391\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.023206 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 48940\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 192\n",
      "[LightGBM] [Info] Start training from score 7.765206\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.19752\tvalid_1's rmse: 1.27027\n",
      "[200]\ttraining's rmse: 1.13694\tvalid_1's rmse: 1.25097\n",
      "[300]\ttraining's rmse: 1.09571\tvalid_1's rmse: 1.24405\n",
      "[400]\ttraining's rmse: 1.05948\tvalid_1's rmse: 1.24103\n",
      "[500]\ttraining's rmse: 1.02714\tvalid_1's rmse: 1.23909\n",
      "[600]\ttraining's rmse: 0.998043\tvalid_1's rmse: 1.2391\n",
      "[700]\ttraining's rmse: 0.969612\tvalid_1's rmse: 1.23811\n",
      "[800]\ttraining's rmse: 0.943109\tvalid_1's rmse: 1.23646\n",
      "[900]\ttraining's rmse: 0.916317\tvalid_1's rmse: 1.23648\n",
      "Early stopping, best iteration is:\n",
      "[814]\ttraining's rmse: 0.939111\tvalid_1's rmse: 1.23604\n",
      "fold2 train RMSE: 0.939, seen val RMSE: 1.171, unseen val RMSE: 1.476\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.025326 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 48946\n",
      "[LightGBM] [Info] Number of data points in the train set: 109121, number of used features: 192\n",
      "[LightGBM] [Info] Start training from score 7.768917\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.19441\tvalid_1's rmse: 1.24599\n",
      "[200]\ttraining's rmse: 1.13388\tvalid_1's rmse: 1.22388\n",
      "[300]\ttraining's rmse: 1.0947\tvalid_1's rmse: 1.21966\n",
      "[400]\ttraining's rmse: 1.05954\tvalid_1's rmse: 1.21671\n",
      "[500]\ttraining's rmse: 1.02675\tvalid_1's rmse: 1.21509\n",
      "[600]\ttraining's rmse: 0.996387\tvalid_1's rmse: 1.21502\n",
      "[700]\ttraining's rmse: 0.96774\tvalid_1's rmse: 1.21388\n",
      "Early stopping, best iteration is:\n",
      "[666]\ttraining's rmse: 0.977139\tvalid_1's rmse: 1.21349\n",
      "fold3 train RMSE: 0.977, seen val RMSE: 1.173, unseen val RMSE: 1.368\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.026618 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 48944\n",
      "[LightGBM] [Info] Number of data points in the train set: 109122, number of used features: 192\n",
      "[LightGBM] [Info] Start training from score 7.769643\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's rmse: 1.18494\tvalid_1's rmse: 1.31485\n",
      "[200]\ttraining's rmse: 1.12548\tvalid_1's rmse: 1.29377\n",
      "[300]\ttraining's rmse: 1.08456\tvalid_1's rmse: 1.28849\n",
      "[400]\ttraining's rmse: 1.05011\tvalid_1's rmse: 1.28611\n",
      "[500]\ttraining's rmse: 1.0183\tvalid_1's rmse: 1.28379\n",
      "[600]\ttraining's rmse: 0.988822\tvalid_1's rmse: 1.28197\n",
      "[700]\ttraining's rmse: 0.961018\tvalid_1's rmse: 1.28059\n",
      "[800]\ttraining's rmse: 0.936456\tvalid_1's rmse: 1.28023\n",
      "[900]\ttraining's rmse: 0.912143\tvalid_1's rmse: 1.28027\n",
      "[1000]\ttraining's rmse: 0.887828\tvalid_1's rmse: 1.27989\n",
      "[1100]\ttraining's rmse: 0.865364\tvalid_1's rmse: 1.27986\n",
      "Early stopping, best iteration is:\n",
      "[1071]\ttraining's rmse: 0.871814\tvalid_1's rmse: 1.27964\n",
      "fold4 train RMSE: 0.872, seen val RMSE: 1.179, unseen val RMSE: 1.633\n",
      "Total RMSE: 1.2344897129455514 | Seen RMSE: 1.1763890457935797 | Unseen RMSE: 1.4517705878593665\n",
      "[Training and evaluation with LightGBM] done in 96 s\n"
     ]
    }
   ],
   "source": [
    "with timer(\"Load the data\"):\n",
    "    train_df, test_df, submission_df = load_data()\n",
    "\n",
    "with timer(\"Stratified & Group split\"):\n",
    "    train_df = stratified_and_group_kfold_split(train_df)\n",
    "\n",
    "with timer(\"add_w2v_features_without_score\"):\n",
    "    # testの視聴情報も活用するため、trainとtestを結合して先に特徴量を作成\n",
    "    train_test_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "    train_test_df = add_w2v_features_without_score(train_test_df)\n",
    "    train_df = train_test_df[train_test_df['score'] != 0].copy().reset_index(drop=True)\n",
    "    test_df = train_test_df[train_test_df['score'] == 0].copy().reset_index(drop=True)\n",
    "\n",
    "with timer(\"Training and evaluation with LightGBM\"):\n",
    "    trained_df, tested_df, sub_df = train(train_df, test_df, submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/working/anime2vec/train_anime2vec.csv\n"
     ]
    }
   ],
   "source": [
    "train_path = os.path.join(\"/workspace\", \"working\", \"anime2vec\",'train_anime2vec.csv')\n",
    "print(train_path)\n",
    "trained_df.to_csv(train_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/working/anime2vec/test_anime2vec.csv\n"
     ]
    }
   ],
   "source": [
    "test_path = os.path.join(\"/workspace\", \"working\", \"anime2vec\",'test_anime2vec.csv')\n",
    "print(test_path)\n",
    "tested_df.to_csv(test_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/working/anime2vec/submission.csv\n"
     ]
    }
   ],
   "source": [
    "sub_path = os.path.join(\"/workspace\", \"working\", \"anime2vec\",'submission.csv')\n",
    "print(sub_path)\n",
    "sub_df.to_csv(sub_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>anime_id</th>\n",
       "      <th>score</th>\n",
       "      <th>fold</th>\n",
       "      <th>wo_score_user_factor_0</th>\n",
       "      <th>wo_score_user_factor_1</th>\n",
       "      <th>wo_score_user_factor_2</th>\n",
       "      <th>wo_score_user_factor_3</th>\n",
       "      <th>wo_score_user_factor_4</th>\n",
       "      <th>wo_score_user_factor_5</th>\n",
       "      <th>...</th>\n",
       "      <th>item_factor_54</th>\n",
       "      <th>item_factor_55</th>\n",
       "      <th>item_factor_56</th>\n",
       "      <th>item_factor_57</th>\n",
       "      <th>item_factor_58</th>\n",
       "      <th>item_factor_59</th>\n",
       "      <th>item_factor_60</th>\n",
       "      <th>item_factor_61</th>\n",
       "      <th>item_factor_62</th>\n",
       "      <th>item_factor_63</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008e10fb39e55447333</td>\n",
       "      <td>04068820a73e52dc3b32</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.186446</td>\n",
       "      <td>-0.056671</td>\n",
       "      <td>0.320237</td>\n",
       "      <td>-0.156542</td>\n",
       "      <td>-0.093355</td>\n",
       "      <td>-0.408961</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.563877</td>\n",
       "      <td>0.207953</td>\n",
       "      <td>-1.429638</td>\n",
       "      <td>-0.585801</td>\n",
       "      <td>1.282804</td>\n",
       "      <td>-0.558889</td>\n",
       "      <td>0.049126</td>\n",
       "      <td>-0.817434</td>\n",
       "      <td>-0.213285</td>\n",
       "      <td>1.126903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0008e10fb39e55447333</td>\n",
       "      <td>04a3d0b122b24965e909</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.186446</td>\n",
       "      <td>-0.056671</td>\n",
       "      <td>0.320237</td>\n",
       "      <td>-0.156542</td>\n",
       "      <td>-0.093355</td>\n",
       "      <td>-0.408961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545700</td>\n",
       "      <td>0.441194</td>\n",
       "      <td>-0.306278</td>\n",
       "      <td>-1.848849</td>\n",
       "      <td>-1.776242</td>\n",
       "      <td>-0.241650</td>\n",
       "      <td>0.265209</td>\n",
       "      <td>-1.163607</td>\n",
       "      <td>-0.664191</td>\n",
       "      <td>0.412600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0008e10fb39e55447333</td>\n",
       "      <td>1447fe1f10b59912d6a8</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.186446</td>\n",
       "      <td>-0.056671</td>\n",
       "      <td>0.320237</td>\n",
       "      <td>-0.156542</td>\n",
       "      <td>-0.093355</td>\n",
       "      <td>-0.408961</td>\n",
       "      <td>...</td>\n",
       "      <td>2.392855</td>\n",
       "      <td>-0.201189</td>\n",
       "      <td>-0.178445</td>\n",
       "      <td>-0.972658</td>\n",
       "      <td>-0.744394</td>\n",
       "      <td>1.533357</td>\n",
       "      <td>0.236169</td>\n",
       "      <td>1.386148</td>\n",
       "      <td>-0.019179</td>\n",
       "      <td>-0.890275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0008e10fb39e55447333</td>\n",
       "      <td>2622632598c68682afd5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.186446</td>\n",
       "      <td>-0.056671</td>\n",
       "      <td>0.320237</td>\n",
       "      <td>-0.156542</td>\n",
       "      <td>-0.093355</td>\n",
       "      <td>-0.408961</td>\n",
       "      <td>...</td>\n",
       "      <td>1.502723</td>\n",
       "      <td>-0.782589</td>\n",
       "      <td>0.452808</td>\n",
       "      <td>-0.044885</td>\n",
       "      <td>-1.627218</td>\n",
       "      <td>-1.846785</td>\n",
       "      <td>-0.043139</td>\n",
       "      <td>0.572776</td>\n",
       "      <td>0.613713</td>\n",
       "      <td>1.474543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0008e10fb39e55447333</td>\n",
       "      <td>2701850c7216516fec46</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.186446</td>\n",
       "      <td>-0.056671</td>\n",
       "      <td>0.320237</td>\n",
       "      <td>-0.156542</td>\n",
       "      <td>-0.093355</td>\n",
       "      <td>-0.408961</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.116279</td>\n",
       "      <td>-1.758816</td>\n",
       "      <td>-1.034489</td>\n",
       "      <td>1.472120</td>\n",
       "      <td>-0.186479</td>\n",
       "      <td>-2.805167</td>\n",
       "      <td>-0.891380</td>\n",
       "      <td>0.698424</td>\n",
       "      <td>-0.689170</td>\n",
       "      <td>-0.770951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117671</th>\n",
       "      <td>ffe85a36cd20500faa58</td>\n",
       "      <td>f508b02efeac8ecb8cc0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234634</td>\n",
       "      <td>-0.268351</td>\n",
       "      <td>0.334127</td>\n",
       "      <td>-0.424138</td>\n",
       "      <td>-0.196915</td>\n",
       "      <td>-0.348194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634340</td>\n",
       "      <td>1.317287</td>\n",
       "      <td>-1.368719</td>\n",
       "      <td>-1.176869</td>\n",
       "      <td>0.324899</td>\n",
       "      <td>0.304721</td>\n",
       "      <td>-2.378378</td>\n",
       "      <td>0.387789</td>\n",
       "      <td>-0.123941</td>\n",
       "      <td>-0.848298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117672</th>\n",
       "      <td>ffe85a36cd20500faa58</td>\n",
       "      <td>f5b8ecea3beea4b82d79</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234634</td>\n",
       "      <td>-0.268351</td>\n",
       "      <td>0.334127</td>\n",
       "      <td>-0.424138</td>\n",
       "      <td>-0.196915</td>\n",
       "      <td>-0.348194</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.215328</td>\n",
       "      <td>-0.631272</td>\n",
       "      <td>1.369881</td>\n",
       "      <td>0.291304</td>\n",
       "      <td>0.035983</td>\n",
       "      <td>-0.146845</td>\n",
       "      <td>-0.774278</td>\n",
       "      <td>0.235659</td>\n",
       "      <td>0.720971</td>\n",
       "      <td>0.037424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117673</th>\n",
       "      <td>ffe85a36cd20500faa58</td>\n",
       "      <td>f6c208226b6b69948053</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234634</td>\n",
       "      <td>-0.268351</td>\n",
       "      <td>0.334127</td>\n",
       "      <td>-0.424138</td>\n",
       "      <td>-0.196915</td>\n",
       "      <td>-0.348194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.893342</td>\n",
       "      <td>1.053601</td>\n",
       "      <td>0.918209</td>\n",
       "      <td>1.750248</td>\n",
       "      <td>-0.969310</td>\n",
       "      <td>0.745615</td>\n",
       "      <td>0.485333</td>\n",
       "      <td>-0.431630</td>\n",
       "      <td>0.505872</td>\n",
       "      <td>-0.186105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117674</th>\n",
       "      <td>ffe85a36cd20500faa58</td>\n",
       "      <td>fe67592c312fc1e17745</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234634</td>\n",
       "      <td>-0.268351</td>\n",
       "      <td>0.334127</td>\n",
       "      <td>-0.424138</td>\n",
       "      <td>-0.196915</td>\n",
       "      <td>-0.348194</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.914432</td>\n",
       "      <td>0.350686</td>\n",
       "      <td>1.212794</td>\n",
       "      <td>-1.586724</td>\n",
       "      <td>-0.282101</td>\n",
       "      <td>0.451981</td>\n",
       "      <td>-0.131902</td>\n",
       "      <td>-1.268467</td>\n",
       "      <td>0.140240</td>\n",
       "      <td>1.535510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117675</th>\n",
       "      <td>ffe85a36cd20500faa58</td>\n",
       "      <td>ff73475b68001c5e533d</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.234634</td>\n",
       "      <td>-0.268351</td>\n",
       "      <td>0.334127</td>\n",
       "      <td>-0.424138</td>\n",
       "      <td>-0.196915</td>\n",
       "      <td>-0.348194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016467</td>\n",
       "      <td>0.366459</td>\n",
       "      <td>0.712465</td>\n",
       "      <td>-1.111156</td>\n",
       "      <td>0.604902</td>\n",
       "      <td>0.274601</td>\n",
       "      <td>0.507034</td>\n",
       "      <td>-0.152836</td>\n",
       "      <td>1.193859</td>\n",
       "      <td>1.541377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117676 rows × 196 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user_id              anime_id  score  fold  \\\n",
       "0       0008e10fb39e55447333  04068820a73e52dc3b32      0   NaN   \n",
       "1       0008e10fb39e55447333  04a3d0b122b24965e909      0   NaN   \n",
       "2       0008e10fb39e55447333  1447fe1f10b59912d6a8      0   NaN   \n",
       "3       0008e10fb39e55447333  2622632598c68682afd5      0   NaN   \n",
       "4       0008e10fb39e55447333  2701850c7216516fec46      0   NaN   \n",
       "...                      ...                   ...    ...   ...   \n",
       "117671  ffe85a36cd20500faa58  f508b02efeac8ecb8cc0      0   NaN   \n",
       "117672  ffe85a36cd20500faa58  f5b8ecea3beea4b82d79      0   NaN   \n",
       "117673  ffe85a36cd20500faa58  f6c208226b6b69948053      0   NaN   \n",
       "117674  ffe85a36cd20500faa58  fe67592c312fc1e17745      0   NaN   \n",
       "117675  ffe85a36cd20500faa58  ff73475b68001c5e533d      0   NaN   \n",
       "\n",
       "        wo_score_user_factor_0  wo_score_user_factor_1  \\\n",
       "0                     0.186446               -0.056671   \n",
       "1                     0.186446               -0.056671   \n",
       "2                     0.186446               -0.056671   \n",
       "3                     0.186446               -0.056671   \n",
       "4                     0.186446               -0.056671   \n",
       "...                        ...                     ...   \n",
       "117671                0.234634               -0.268351   \n",
       "117672                0.234634               -0.268351   \n",
       "117673                0.234634               -0.268351   \n",
       "117674                0.234634               -0.268351   \n",
       "117675                0.234634               -0.268351   \n",
       "\n",
       "        wo_score_user_factor_2  wo_score_user_factor_3  \\\n",
       "0                     0.320237               -0.156542   \n",
       "1                     0.320237               -0.156542   \n",
       "2                     0.320237               -0.156542   \n",
       "3                     0.320237               -0.156542   \n",
       "4                     0.320237               -0.156542   \n",
       "...                        ...                     ...   \n",
       "117671                0.334127               -0.424138   \n",
       "117672                0.334127               -0.424138   \n",
       "117673                0.334127               -0.424138   \n",
       "117674                0.334127               -0.424138   \n",
       "117675                0.334127               -0.424138   \n",
       "\n",
       "        wo_score_user_factor_4  wo_score_user_factor_5  ...  item_factor_54  \\\n",
       "0                    -0.093355               -0.408961  ...       -0.563877   \n",
       "1                    -0.093355               -0.408961  ...        0.545700   \n",
       "2                    -0.093355               -0.408961  ...        2.392855   \n",
       "3                    -0.093355               -0.408961  ...        1.502723   \n",
       "4                    -0.093355               -0.408961  ...       -1.116279   \n",
       "...                        ...                     ...  ...             ...   \n",
       "117671               -0.196915               -0.348194  ...        0.634340   \n",
       "117672               -0.196915               -0.348194  ...       -1.215328   \n",
       "117673               -0.196915               -0.348194  ...        0.893342   \n",
       "117674               -0.196915               -0.348194  ...       -0.914432   \n",
       "117675               -0.196915               -0.348194  ...        0.016467   \n",
       "\n",
       "        item_factor_55  item_factor_56  item_factor_57  item_factor_58  \\\n",
       "0             0.207953       -1.429638       -0.585801        1.282804   \n",
       "1             0.441194       -0.306278       -1.848849       -1.776242   \n",
       "2            -0.201189       -0.178445       -0.972658       -0.744394   \n",
       "3            -0.782589        0.452808       -0.044885       -1.627218   \n",
       "4            -1.758816       -1.034489        1.472120       -0.186479   \n",
       "...                ...             ...             ...             ...   \n",
       "117671        1.317287       -1.368719       -1.176869        0.324899   \n",
       "117672       -0.631272        1.369881        0.291304        0.035983   \n",
       "117673        1.053601        0.918209        1.750248       -0.969310   \n",
       "117674        0.350686        1.212794       -1.586724       -0.282101   \n",
       "117675        0.366459        0.712465       -1.111156        0.604902   \n",
       "\n",
       "        item_factor_59  item_factor_60  item_factor_61  item_factor_62  \\\n",
       "0            -0.558889        0.049126       -0.817434       -0.213285   \n",
       "1            -0.241650        0.265209       -1.163607       -0.664191   \n",
       "2             1.533357        0.236169        1.386148       -0.019179   \n",
       "3            -1.846785       -0.043139        0.572776        0.613713   \n",
       "4            -2.805167       -0.891380        0.698424       -0.689170   \n",
       "...                ...             ...             ...             ...   \n",
       "117671        0.304721       -2.378378        0.387789       -0.123941   \n",
       "117672       -0.146845       -0.774278        0.235659        0.720971   \n",
       "117673        0.745615        0.485333       -0.431630        0.505872   \n",
       "117674        0.451981       -0.131902       -1.268467        0.140240   \n",
       "117675        0.274601        0.507034       -0.152836        1.193859   \n",
       "\n",
       "        item_factor_63  \n",
       "0             1.126903  \n",
       "1             0.412600  \n",
       "2            -0.890275  \n",
       "3             1.474543  \n",
       "4            -0.770951  \n",
       "...                ...  \n",
       "117671       -0.848298  \n",
       "117672        0.037424  \n",
       "117673       -0.186105  \n",
       "117674        1.535510  \n",
       "117675        1.541377  \n",
       "\n",
       "[117676 rows x 196 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tested_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
