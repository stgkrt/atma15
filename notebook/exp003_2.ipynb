{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pickle\n",
    "from glob import glob\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import wandb\n",
    "from wandb.lightgbm import wandb_callback, log_summary\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"exp003_2\"\n",
    "# EXP_NAME = \"debug\"\n",
    "\n",
    "class configs:\n",
    "    EXP_CATEGORY = \"baseline\"\n",
    "    EXP_NAME = EXP_NAME\n",
    "    OUTPUT_DIR = os.path.join(\"/workspace\", \"working\", EXP_NAME)\n",
    "    \n",
    "    INPUT_DIR = os.path.join(\"/workspace\", \"input\", \"atmaCup15_dataset\")\n",
    "    # TRAIN_CSV = \"/workspace/working/svd_filtering_oof/train_addSVD.csv\"\n",
    "    TRAIN_CSV = \"/workspace/working/svdpp/train_addSVDpp.csv\"\n",
    "    ANIME_CSV = os.path.join(INPUT_DIR, \"anime.csv\")\n",
    "    # TEST_CSV = \"/workspace/working/svd_filtering_oof/test_addSVD.csv\"\n",
    "    TEST_CSV = \"/workspace/working/svdpp/test_addSVDpp.csv\"\n",
    "    SAMPLE_SUB_CSV = os.path.join(INPUT_DIR, \"sample_submission.csv\")\n",
    "    target_colname = \"score\"\n",
    "    \n",
    "    COMPETITION = \"atmaCup15\"\n",
    "    USER_NAME = \"taro\"\n",
    "    wandb_available = True\n",
    "    \n",
    "    # train\n",
    "    num_boost_round = 20000\n",
    "    early_stopping_rounds = 500\n",
    "    verbose_eval = 1000\n",
    "    FOLDS = [0, 1, 2, 3, 4]\n",
    "\n",
    "if EXP_NAME is \"debug\":\n",
    "    configs.wandb_available = False\n",
    "    configs.num_boost_round = 10\n",
    "else:\n",
    "    os.makedirs(configs.OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'dart',\n",
    "    'objective': 'regression',\n",
    "    \"objective\": \"rmse\", \n",
    "    'metric': 'rmse',\n",
    "    \"n_estimators\": 20000, \n",
    "    \"learning_rate\": .01,\n",
    "    \"verbosity\": -1, \n",
    "    \"random_state\": 510,\n",
    "}\n",
    "configs.params = params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __init__(self, logger=None, format_str=\"{:.3f}[s]\", prefix=None, suffix=None, sep=\" \"):\n",
    "\n",
    "        if prefix: format_str = str(prefix) + sep + format_str\n",
    "        if suffix: format_str = format_str + sep + str(suffix)\n",
    "        self.format_str = format_str\n",
    "        self.logger = logger\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end is None:\n",
    "            return 0\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time()\n",
    "        out_str = self.format_str.format(self.duration)\n",
    "        if self.logger:\n",
    "            self.logger.info(out_str)\n",
    "        else:\n",
    "            print(out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"mean_squared_error の root (0.5乗)\"\"\"\n",
    "    return mean_squared_error(y_true, y_pred) ** .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_by_anime_id(left_df, right_df):\n",
    "    return pd.merge(left_df[\"anime_id\"], right_df, on=\"anime_id\", how=\"left\").drop(columns=[\"anime_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## features func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_numeric_feature(input_df: pd.DataFrame):\n",
    "    \"\"\"input_dfは train or test.csv のデータが入ってくることを想定しています.\"\"\"\n",
    "    \n",
    "    use_columns = [\n",
    "        \"members\", \n",
    "    ]\n",
    "    \n",
    "    return merge_by_anime_id(input_df, anime_df)[use_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_genres_label_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"genres\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_le\": encoder.fit_transform(anime_df[target_col].fillna(\"nan\"))\n",
    "    })\n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_source_label_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"source\"\n",
    "    encoder = LabelEncoder()\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_le\": encoder.fit_transform(anime_df[target_col].fillna(\"nan\"))\n",
    "    })\n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### count encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをカウントエンコーディング\n",
    "def create_anime_type_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"type\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_studios_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"studios\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_producers_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"producers\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anime_animeid_count_encoding(input_df: pd.DataFrame):\n",
    "    target_col = \"anime_id\"\n",
    "    count = anime_df[target_col].map(anime_df[\"type\"].value_counts())\n",
    "    encoded_df = pd.DataFrame({\n",
    "        \"anime_id\": anime_df[\"anime_id\"],\n",
    "        f\"{target_col}_count\": count\n",
    "    })\n",
    "    \n",
    "    return merge_by_anime_id(input_df, encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをone-hotエンコーディング\n",
    "def create_anime_type_one_hot_encoding(input_df: pd.DataFrame):\n",
    "    # 対象の列のユニーク集合を取る\n",
    "    target_colname = \"type\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    # ユニークな値ごとに列を作る\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        if value == \"Unknown\":\n",
    "            out_df[\"Unknown_type\"] = is_value.astype(int)\n",
    "        else:\n",
    "            out_df[value] = is_value.astype(int)\n",
    "    \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    return merge_by_anime_id(input_df, out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# animeのtypeをone-hotエンコーディング\n",
    "def create_anime_rating_one_hot_encoding(input_df: pd.DataFrame):\n",
    "    # 対象の列のユニーク集合を取る\n",
    "    target_colname = \"rating\"\n",
    "    target_series = anime_df[target_colname]\n",
    "    unique_values = target_series.unique()\n",
    "\n",
    "    # ユニークな値ごとに列を作る\n",
    "    out_df = pd.DataFrame()\n",
    "    for value in unique_values:\n",
    "        is_value = target_series == value\n",
    "        if value == \"Unknown\":\n",
    "            out_df[\"Unknown_rate\"] = is_value.astype(int)\n",
    "        else:\n",
    "            out_df[value] = is_value.astype(int)\n",
    "    \n",
    "    out_df[\"anime_id\"] = anime_df[\"anime_id\"]\n",
    "    return merge_by_anime_id(input_df, out_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上で定義した関数をまとめて実行\n",
    "def create_feature(input_df, config_):\n",
    "    # functions に特徴量作成関数を配列で定義しました.\n",
    "    # どの関数も同じ input / output のインターフェイスなので for で回せて嬉しいですね ;)\n",
    "    functions = [\n",
    "        create_anime_numeric_feature,\n",
    "        # label encoding\n",
    "        create_anime_genres_label_encoding, \n",
    "        create_anime_source_label_encoding, \n",
    "        # count encoding\n",
    "        create_anime_type_count_encoding,\n",
    "        create_anime_studios_count_encoding,\n",
    "        create_anime_producers_count_encoding,\n",
    "        create_anime_animeid_count_encoding, \n",
    "        # one-hot encoding\n",
    "        create_anime_type_one_hot_encoding,\n",
    "        create_anime_rating_one_hot_encoding,\n",
    "    ]\n",
    "    \n",
    "    out_df = pd.DataFrame()\n",
    "    func_name_list = []\n",
    "    for func in functions:\n",
    "        func_name = str(func.__name__)\n",
    "        func_name_list.append(func_name)\n",
    "        with Timer(prefix=f\"create {func_name}\"):\n",
    "            _df = func(input_df)\n",
    "        out_df = pd.concat([out_df, _df], axis=1)\n",
    "    \n",
    "    config_.preprocess_funcs = func_name_list\n",
    "    return out_df, config_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, fold):\n",
    "    save_path = os.path.join(configs.OUTPUT_DIR, f\"model_fold{fold}.pkl\")\n",
    "    # pickle.dump(model, save_path)\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"SAVED: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lgbm(df, configs):\n",
    "    \"\"\"lightGBM を CrossValidation の枠組みで学習を行なう function\"\"\"\n",
    "\n",
    "    models = []\n",
    "    evals_results_list = [] \n",
    "    n_records = len(df)\n",
    "    # training data の target と同じだけのゼロ配列を用意\n",
    "    oof_pred = np.zeros((n_records, ), dtype=np.float32)\n",
    "    target = []\n",
    "    for fold in configs.FOLDS: \n",
    "        # この部分が交差検証のところです。データセットを cv instance によって分割します\n",
    "        # training data を trian/valid に分割\n",
    "        train_df_ = df[df[\"fold\"] != fold].reset_index(drop=True)\n",
    "        valid_df_ = df[df[\"fold\"] == fold].reset_index(drop=True)\n",
    "        idx_valid = df[df[\"fold\"] == fold].index.values\n",
    "        \n",
    "        x_train = train_df_.drop(columns=[configs.target_colname, \"fold\"])\n",
    "        # print(x_train.columns)\n",
    "        y_train = train_df_[configs.target_colname]\n",
    "        x_valid = valid_df_.drop(columns=[configs.target_colname, \"fold\"])\n",
    "        y_valid = valid_df_[configs.target_colname]\n",
    "        target.extend(y_valid)\n",
    "        \n",
    "        lgb_train = lgb.Dataset(x_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(x_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "        lgb_result = {}\n",
    "        with Timer(prefix=\"fit fold={} \".format(fold)):\n",
    "            if configs.wandb_available:\n",
    "                clf = lgb.train(configs.params,\n",
    "                                lgb_train,\n",
    "                                valid_sets=[lgb_train, lgb_eval],  \n",
    "                                valid_names=[f\"validation_{fold}\"],\n",
    "                                num_boost_round=configs.num_boost_round,\n",
    "                                early_stopping_rounds=configs.early_stopping_rounds,\n",
    "                                evals_result=lgb_result,\n",
    "                                verbose_eval=configs.verbose_eval,\n",
    "                                callbacks=[wandb_callback()])\n",
    "                evals_results_list.append(lgb_result)\n",
    "                log_summary(clf, save_model_checkpoint=False)\n",
    "\n",
    "            else:\n",
    "                clf = lgb.train(configs.params,\n",
    "                                lgb_train,\n",
    "                                valid_sets=[lgb_train, lgb_eval],  \n",
    "                                valid_names=[f\"validation_{fold}\"],\n",
    "                                num_boost_round=configs.num_boost_round,\n",
    "                                early_stopping_rounds=configs.early_stopping_rounds,\n",
    "                                evals_result=lgb_result,\n",
    "                                verbose_eval=configs.verbose_eval,\n",
    "                                )\n",
    "                evals_results_list.append(lgb_result)\n",
    "        \n",
    "        # cv 内で validation data とされた x_valid で予測をして oof_pred に保存していく\n",
    "        # oof_pred は全部学習に使わなかったデータの予測結果になる → モデルの予測性能を見る指標として利用できる\n",
    "        pred_i = clf.predict(x_valid)\n",
    "        oof_pred[idx_valid] = pred_i\n",
    "        models.append(clf)\n",
    "        save_model(clf, fold)\n",
    "        score = root_mean_squared_error(y_valid, pred_i)\n",
    "        print(f\" - fold{fold} - {score:.4f}\")\n",
    "        if configs.wandb_available:\n",
    "            wandb.log({\"fold\": fold, \"rmse\": score})\n",
    "\n",
    "    score = root_mean_squared_error(target, oof_pred)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"FINISHI: Whole Score: {score:.4f}\")\n",
    "    return oof_pred, target, models, evals_results_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df = pd.read_csv(configs.ANIME_CSV)\n",
    "\n",
    "train_df = pd.read_csv(configs.TRAIN_CSV)\n",
    "test_df = pd.read_csv(configs.TEST_CSV)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess\n",
    "- CountEncoding\n",
    "- OneHotEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create create_anime_numeric_feature 0.051[s]\n",
      "create create_anime_genres_label_encoding 0.012[s]\n",
      "create create_anime_source_label_encoding 0.010[s]\n",
      "create create_anime_type_count_encoding 0.011[s]\n",
      "create create_anime_studios_count_encoding 0.011[s]\n",
      "create create_anime_producers_count_encoding 0.011[s]\n",
      "create create_anime_animeid_count_encoding 0.010[s]\n",
      "create create_anime_type_one_hot_encoding 0.014[s]\n",
      "create create_anime_rating_one_hot_encoding 0.016[s]\n",
      "train... 0.155[s]\n",
      "create create_anime_numeric_feature 0.043[s]\n",
      "create create_anime_genres_label_encoding 0.011[s]\n",
      "create create_anime_source_label_encoding 0.009[s]\n",
      "create create_anime_type_count_encoding 0.010[s]\n",
      "create create_anime_studios_count_encoding 0.010[s]\n",
      "create create_anime_producers_count_encoding 0.009[s]\n",
      "create create_anime_animeid_count_encoding 0.013[s]\n",
      "create create_anime_type_one_hot_encoding 0.014[s]\n",
      "create create_anime_rating_one_hot_encoding 0.014[s]\n",
      "test... 0.145[s]\n"
     ]
    }
   ],
   "source": [
    "# 実行して train / test 用の特徴量を作ります.\n",
    "\n",
    "with Timer(prefix=\"train...\"):\n",
    "    train_feat_df, configs = create_feature(train_df, configs)\n",
    "\n",
    "with Timer(prefix=\"test...\"):\n",
    "    test_feat_df, configs = create_feature(test_df, configs)\n",
    "\n",
    "# X = train_feat_df.values\n",
    "# print(train_feat_df.columns)\n",
    "# y = train_df[\"score\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = pd.concat([train_df, train_feat_df], axis=1)\n",
    "input_df = input_df.drop([\"user_id\", \"anime_id\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['score', 'fold', 'svd', 'members', 'genres_le', 'source_le',\n",
       "       'type_count', 'studios_count', 'producers_count', 'anime_id_count',\n",
       "       'TV', 'Special', 'Movie', 'Unknown_type', 'ONA', 'OVA', 'Music',\n",
       "       'PG-13 - Teens 13 or older', 'R+ - Mild Nudity',\n",
       "       'R - 17+ (violence & profanity)', 'G - All Ages', 'PG - Children',\n",
       "       'Rx - Hentai', 'Unknown_rate'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb initialized\n"
     ]
    }
   ],
   "source": [
    "if configs.wandb_available:\n",
    "   WANDB_CONFIG = {'competition': \"atma15\", '_wandb_kernel': \"taro\"}\n",
    "   os.environ[\"WANDB_SILENT\"] = \"true\"\n",
    "   # config_dict = dict(vars(configs))\n",
    "   wandb.init(project=WANDB_CONFIG[\"competition\"],\n",
    "               # config=config_dict,\n",
    "               group=configs.EXP_CATEGORY, \n",
    "               name=configs.EXP_NAME,\n",
    "               reinit=True,\n",
    "               save_code=True)\n",
    "   print(\"wandb initialized\")\n",
    "else:\n",
    "   print(\"wandb logging is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalidation_0's rmse: 1.71719\tvalid_1's rmse: 1.72277\n",
      "[2000]\tvalidation_0's rmse: 1.31775\tvalid_1's rmse: 1.33103\n",
      "[3000]\tvalidation_0's rmse: 1.24524\tvalid_1's rmse: 1.26451\n",
      "[4000]\tvalidation_0's rmse: 1.20485\tvalid_1's rmse: 1.22992\n",
      "[5000]\tvalidation_0's rmse: 1.17992\tvalid_1's rmse: 1.21092\n",
      "[6000]\tvalidation_0's rmse: 1.17847\tvalid_1's rmse: 1.21355\n",
      "[7000]\tvalidation_0's rmse: 1.16486\tvalid_1's rmse: 1.20511\n",
      "[8000]\tvalidation_0's rmse: 1.16033\tvalid_1's rmse: 1.20468\n",
      "[9000]\tvalidation_0's rmse: 1.15505\tvalid_1's rmse: 1.20374\n",
      "[10000]\tvalidation_0's rmse: 1.14963\tvalid_1's rmse: 1.20282\n",
      "[11000]\tvalidation_0's rmse: 1.14596\tvalid_1's rmse: 1.20305\n",
      "[12000]\tvalidation_0's rmse: 1.14322\tvalid_1's rmse: 1.20403\n",
      "[13000]\tvalidation_0's rmse: 1.13944\tvalid_1's rmse: 1.20406\n",
      "[14000]\tvalidation_0's rmse: 1.13597\tvalid_1's rmse: 1.20438\n",
      "[15000]\tvalidation_0's rmse: 1.13322\tvalid_1's rmse: 1.20524\n",
      "[16000]\tvalidation_0's rmse: 1.12988\tvalid_1's rmse: 1.20543\n",
      "[17000]\tvalidation_0's rmse: 1.12799\tvalid_1's rmse: 1.20664\n",
      "[18000]\tvalidation_0's rmse: 1.12445\tvalid_1's rmse: 1.20662\n",
      "[19000]\tvalidation_0's rmse: 1.12213\tvalid_1's rmse: 1.20772\n",
      "[20000]\tvalidation_0's rmse: 1.11944\tvalid_1's rmse: 1.2082\n",
      "fit fold=0  628.166[s]\n",
      "SAVED: /workspace/working/exp003_2/model_fold0.pkl\n",
      " - fold0 - 1.2082\n",
      "fit fold=1  13.796[s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m oof, target, models, evals_results \u001b[39m=\u001b[39m fit_lgbm(input_df, configs)\n",
      "Cell \u001b[0;32mIn[18], line 30\u001b[0m, in \u001b[0;36mfit_lgbm\u001b[0;34m(df, configs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mwith\u001b[39;00m Timer(prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfit fold=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(fold)):\n\u001b[1;32m     29\u001b[0m     \u001b[39mif\u001b[39;00m configs\u001b[39m.\u001b[39mwandb_available:\n\u001b[0;32m---> 30\u001b[0m         clf \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(configs\u001b[39m.\u001b[39;49mparams,\n\u001b[1;32m     31\u001b[0m                         lgb_train,\n\u001b[1;32m     32\u001b[0m                         valid_sets\u001b[39m=\u001b[39;49m[lgb_train, lgb_eval],  \n\u001b[1;32m     33\u001b[0m                         valid_names\u001b[39m=\u001b[39;49m[\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mvalidation_\u001b[39;49m\u001b[39m{\u001b[39;49;00mfold\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     34\u001b[0m                         num_boost_round\u001b[39m=\u001b[39;49mconfigs\u001b[39m.\u001b[39;49mnum_boost_round,\n\u001b[1;32m     35\u001b[0m                         early_stopping_rounds\u001b[39m=\u001b[39;49mconfigs\u001b[39m.\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m     36\u001b[0m                         evals_result\u001b[39m=\u001b[39;49mlgb_result,\n\u001b[1;32m     37\u001b[0m                         verbose_eval\u001b[39m=\u001b[39;49mconfigs\u001b[39m.\u001b[39;49mverbose_eval,\n\u001b[1;32m     38\u001b[0m                         callbacks\u001b[39m=\u001b[39;49m[wandb_callback()])\n\u001b[1;32m     39\u001b[0m         evals_results_list\u001b[39m.\u001b[39mappend(lgb_result)\n\u001b[1;32m     40\u001b[0m         log_summary(clf, save_model_checkpoint\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:292\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39mfor\u001b[39;00m cb \u001b[39min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    285\u001b[0m     cb(callback\u001b[39m.\u001b[39mCallbackEnv(model\u001b[39m=\u001b[39mbooster,\n\u001b[1;32m    286\u001b[0m                             params\u001b[39m=\u001b[39mparams,\n\u001b[1;32m    287\u001b[0m                             iteration\u001b[39m=\u001b[39mi,\n\u001b[1;32m    288\u001b[0m                             begin_iteration\u001b[39m=\u001b[39minit_iteration,\n\u001b[1;32m    289\u001b[0m                             end_iteration\u001b[39m=\u001b[39minit_iteration \u001b[39m+\u001b[39m num_boost_round,\n\u001b[1;32m    290\u001b[0m                             evaluation_result_list\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m))\n\u001b[0;32m--> 292\u001b[0m booster\u001b[39m.\u001b[39;49mupdate(fobj\u001b[39m=\u001b[39;49mfobj)\n\u001b[1;32m    294\u001b[0m evaluation_result_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    295\u001b[0m \u001b[39m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:3021\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3019\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3020\u001b[0m     \u001b[39mraise\u001b[39;00m LightGBMError(\u001b[39m'\u001b[39m\u001b[39mCannot update due to null objective function.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m-> 3021\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_BoosterUpdateOneIter(\n\u001b[1;32m   3022\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   3023\u001b[0m     ctypes\u001b[39m.\u001b[39;49mbyref(is_finished)))\n\u001b[1;32m   3024\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__is_predicted_cur_iter \u001b[39m=\u001b[39m [\u001b[39mFalse\u001b[39;00m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3025\u001b[0m \u001b[39mreturn\u001b[39;00m is_finished\u001b[39m.\u001b[39mvalue \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "oof, target, models, evals_results = fit_lgbm(input_df, configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for evals in evals_results:\n",
    "    lgb.plot_metric(evals, metric=\"rmse\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_score = root_mean_squared_error(y_true=target, y_pred=oof)\n",
    "print(oof_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "sns.boxenplot(data=pd.DataFrame({ \"GroundTruth\": target, \"OutOfFold Prediction\": oof }), \n",
    "              x=\"GroundTruth\", y=\"OutOfFold Prediction\", ax=ax)\n",
    "\n",
    "ax.grid()\n",
    "ax.plot([0, 9], [1, 10], \"--\", c=\"black\", alpha=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model in enumerate(models):\n",
    "    lgb.plot_importance(model, figsize=(5,5),\n",
    "                        importance_type=\"gain\", max_num_features=25, \n",
    "                        xlabel=\"Feature Importance\", ylabel=\"Features\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "['svd', 'members', 'tyoe_count', 'TV', 'Special', 'Movie', 'Unknown','ONA', 'OVA', 'Music']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = [col for col in input_df.columns if col not in [\"fold\", \"score\"]]\n",
    "print(input_columns)\n",
    "print(len(input_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.concat([test_df, test_feat_df], axis=1)\n",
    "test_df = test_df[input_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k 個のモデルの予測を作成. shape = (5, N_test,).\n",
    "pred = np.array([model.predict(test_df.values) for model in models])\n",
    "\n",
    "\n",
    "# k 個のモデルの予測値の平均 shape = (N_test,).\n",
    "pred = np.mean(pred, axis=0) # axis=0 なので shape の `k` が潰れる "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "vmax = .02\n",
    "bins = np.linspace(0, 10, 100)\n",
    "ax.hist(pred, bins=bins, density=True, alpha=.5, label=\"Test\")\n",
    "ax.hist(oof, bins=bins, density=True, alpha=.5, label=\"OutOfFold\")\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "# ax.set_title(\"テストと学習時の予測傾向差分\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save infer csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\n",
    "    \"score\": pred\n",
    "}).to_csv(os.path.join(configs.OUTPUT_DIR, \"submission.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if configs.wandb_available:\n",
    "    wandb.log({\"oof_score\": oof_score})\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
